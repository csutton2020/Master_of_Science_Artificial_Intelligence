{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Christian Sutton\n",
    "11/16/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 1\n",
    "**[50 pts] In this assignment, we will use Apriori analysis to find noun phrases, or interesting patterns in a novel.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv file of Alice txt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "# # nltk.download('stopwords')\n",
    "# # nltk.download('gutenberg')\n",
    "# # nltk.download('punkt')\n",
    "# Stop_words = stopwords.words('english')\n",
    "# Sentences = gutenberg.sents('carroll-alice.txt')\n",
    "# TermsSentences = []\n",
    "# for terms in Sentences:\n",
    "#     terms = [w for w in terms if w not in Stop_words]\n",
    "#     terms = [w for w in terms if re.search(r'^[a-zA-Z]{2}', w) is not None]\n",
    "#     TermsSentences+= [terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store file for module 10\n",
    "# pd.DataFrame(TermsSentences).to_csv(\"C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod11/AliceTxt.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=2793 items, N=1703 transactions\n"
     ]
    }
   ],
   "source": [
    "#normalize data using module 10\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from itertools import combinations \n",
    "import numpy as np\n",
    "\n",
    "Transactions_list = []  # a list of transactions\n",
    "Items_names = {}  # Lookup item ID to name\n",
    "Items_ids = {}  # Lookup item name to ID\n",
    "\n",
    "Items = None  # a list of item IDs, normally an increasing sequence of numbers\n",
    "\n",
    "# Process the data\n",
    "with open('C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod11/AliceTxt.csv', 'r') as fin:\n",
    "    reader = csv.reader(fin, delimiter=',')\n",
    "    item_id = 0\n",
    "    for row in reader:\n",
    "        transaction = []\n",
    "        for item in row:\n",
    "            if item not in Items_ids:\n",
    "                Items_ids[item] = item_id\n",
    "                Items_names[item_id] = item\n",
    "                item_id += 1\n",
    "            #\n",
    "            transaction += [Items_ids[item]]\n",
    "        #\n",
    "        Transactions_list += [transaction]\n",
    "\n",
    "M, N = len(Items_ids), len(Transactions_list)\n",
    "\n",
    "Items = np.arange(0,M)\n",
    "\n",
    "# Information\n",
    "print(f'M={M} items, N={N} transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice', 'Adventures', 'Wonderland', 'Lewis', 'Carroll', 'CHAPTER', 'Down']\n",
      "[[0, 1, 2, 3, 4], [5], [6, 7, 8], [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 13, 19, 20, 21, 22, 18, 23, 0, 24, 20, 25], [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 7, 50, 51, 52, 53], [54, 15, 55, 56, 0, 57, 55, 58, 59, 60, 7, 61, 62, 63], [62, 63], [64, 65], [23, 66, 67, 68, 69, 70, 71, 72, 73, 7, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 0, 84, 85, 86, 87, 28, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 52, 87, 98, 99, 70, 100, 101, 102, 90, 103, 104], [105, 106, 107, 108, 0, 88, 27, 109, 10], [110, 90, 103, 108, 111, 112, 113, 59, 114, 48, 48, 0, 107, 57, 115, 116, 117, 118, 29], [119, 29, 118, 120, 121, 122, 70, 108, 123, 124, 125, 126, 127], [128, 129, 123, 130, 131, 132, 100, 133, 82, 134, 29, 135, 136, 137, 18, 138, 139, 140, 20, 141, 142, 143], [144, 145, 146, 147, 138, 148, 149, 150, 151, 152, 153, 154, 112, 155, 146, 156, 157, 158, 159, 160, 147, 137, 120, 161], [162], [23, 0, 163, 64, 57, 15, 164, 165], [166, 167, 57, 168], [169, 61, 133, 170, 120, 171, 172], [173, 174, 175], [6], [176, 163, 177, 178, 179], [124, 180, 181, 182, 70], [183, 184], [185, 45, 186, 187, 188, 189], [190, 100, 42, 191, 192, 181, 57, 100, 0, 193, 194, 195, 196, 197, 198, 199, 55, 200, 201, 202, 203, 147, 204, 205, 200, 206, 61, 207, 208, 209, 124, 210, 211, 212], [0, 213, 210, 211, 91, 23, 214, 215, 216, 61], [217, 218], [124, 64, 163, 208, 219, 189], [166, 220, 221, 178, 222, 223, 224, 225, 226], [110, 227, 57, 228, 229, 230, 147, 231, 70, 232, 208, 233, 64, 234, 235, 236, 237], [238, 239, 240, 241, 242], [129, 243, 244, 245, 246, 117, 247], [248, 57, 30, 249], [250, 251, 252, 253, 57, 254], [255, 88, 234, 256, 64, 100, 257, 186], [6], [54, 15, 258, 0, 259, 218, 260], [261, 262, 58, 263, 57], [261, 264], [265, 266, 267, 268, 269, 70], [261, 63], [270], [54, 271, 247, 272, 273, 274, 275, 112, 276, 237], [277, 278, 279, 280, 124], [250, 0, 218, 10, 228, 35, 108, 281, 282, 196, 59, 248, 278, 279, 280], [248, 278, 279, 280], [283, 248, 280, 279, 278], [100, 284, 91, 285, 58, 286, 59, 160], [144, 287, 288, 289, 290, 291, 292, 292, 261, 281, 293, 294, 261, 295, 296, 297, 279, 275], [48, 298], [298], [299, 142, 300, 301, 302, 303, 163], [0, 304, 305, 306, 85, 107, 82, 132, 307, 106, 308, 309, 49, 7, 205, 310, 311], [54, 107, 312, 313, 108, 0, 112, 314, 70, 60, 61, 315, 316, 62, 317, 318, 65, 45], [144, 53, 319, 315, 316, 7, 320, 89, 116, 308, 321, 322, 323, 324, 325, 326, 327], [54, 328, 329, 322, 330, 0, 59, 147, 331, 332, 333, 334, 335, 336, 337, 338, 297, 10], [339, 299, 142, 252, 340, 341, 342, 33, 343, 344, 15, 345, 346, 347, 348, 0, 349, 23, 273, 350, 147, 328, 322, 351], [91, 352, 102, 348, 353, 354, 42, 355], [356, 357, 70, 329, 299, 142, 321, 358, 135, 319, 252, 334, 359, 360, 361, 129, 252, 347, 348, 362, 152, 363, 364], [0, 365, 334, 116, 366, 353, 309, 58, 367, 368, 103, 369, 82, 370, 309, 371, 372, 297, 139], [166, 373, 10, 132, 322, 374, 222, 375, 376, 377, 378, 379, 30, 170, 10, 380, 381, 170, 380, 42, 382, 23, 383, 0, 42, 252, 22, 24, 384], [62, 270, 30, 385, 112, 386], [57, 30, 237, 387], [388, 100, 180, 59, 195, 389, 390, 0, 289, 57, 195, 391, 392, 393], [54, 71, 22, 394, 252, 334, 108, 395, 342, 396, 397, 273, 398, 106, 348, 354, 18, 399, 400, 223, 112, 401, 70, 116, 252, 402, 403, 183, 0, 329, 404, 402, 405, 406, 216, 407, 408, 409, 410, 102, 411], [412, 29, 61, 413, 414, 252, 0, 125, 415, 416], [255, 123, 349, 183, 100, 37, 417, 418, 419, 194, 214, 252, 420, 421, 212, 422, 423, 424, 425, 426, 195, 427, 266, 428, 399, 429, 430, 431, 31, 432, 433, 434, 308, 435, 436, 55, 437, 438, 439, 440, 88, 441, 442, 58, 402, 417, 418, 443, 444, 445, 446, 447], [356, 402, 448, 417, 418, 0, 449, 450, 451, 214, 452, 196, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 31, 463, 464, 259, 465], [], [], [], [466, 467, 468], [183, 0, 185, 400, 112, 386], [250, 391, 469, 360, 361, 470, 471, 23, 208, 472, 125, 252, 334, 473, 372], [128, 474, 475, 476, 100, 125, 477, 287, 252, 478, 273, 179, 237, 183, 0, 125, 479, 112, 480], [124, 112], [250, 129, 245, 481, 480, 112, 480, 482, 30, 266, 297, 89, 483], [484, 451, 15, 389, 485, 125, 372, 351, 383, 0], [212, 334, 116, 441, 252, 347, 348, 108, 395, 342, 116, 30, 486, 487, 30, 100, 72, 488, 344, 129, 489, 490, 147, 491, 342, 492, 11, 332, 383, 252, 483, 493, 494], [495, 22, 496, 112], [183, 0, 228, 497, 498, 499, 500], [144, 501, 502, 200, 503, 199, 504, 505, 283, 506, 507, 508, 509, 51, 510, 332, 511, 317, 512, 513, 514, 515, 467, 516, 517, 518, 519, 223], [277, 22, 23, 383, 0, 520, 519, 223], [169, 521, 522, 523, 130, 524, 525, 526], [527, 528, 120, 252, 344, 511, 529, 342, 365, 116, 353, 530, 216, 531, 408, 409, 417, 532], [162, 279, 183, 0, 533, 534, 367, 487, 348, 533, 534, 535, 536, 334, 91, 59, 10, 372, 537, 538], [144, 539, 252, 304, 183, 540, 173, 59], [173, 59], [541, 292, 171, 380, 34, 59, 542, 72, 543, 398, 544, 472, 545, 501, 538, 147, 546, 530, 0, 212, 58, 59, 547, 15, 59, 195, 126, 71, 72, 548, 36, 549, 382, 550, 59], [26, 551, 552, 259, 465, 530], [], [], [], [5, 553], [110, 554, 555], [556, 557], [494, 0, 58, 543, 107, 72, 558, 559, 200, 560, 561, 112, 562, 386, 297], [563, 564, 85], [82, 85, 71, 443, 310, 45, 565], [62, 383, 252, 85, 124, 160, 566, 567, 568], [545, 569], [64, 152, 570, 565, 44, 185, 249, 489, 59, 185, 571, 23, 0, 256, 224, 59, 572, 382], [190, 100, 573, 574, 575, 576, 333, 577], [250, 108, 578, 42, 249], [579, 185, 382, 580, 23, 220, 221, 581, 582, 147, 85], [250, 583, 584, 123], [585, 586, 587, 588], [589, 590, 591, 592, 593, 585, 594], [62, 63, 595, 260], [596, 380, 597, 327, 322, 452, 598, 85, 361, 145, 252, 347, 348, 83, 372, 334], [599, 0], [412, 58, 30, 529, 147, 331, 123, 372, 147, 528, 10, 600, 297, 493, 218, 601], [602, 68, 603, 183, 0, 152, 253, 112, 273, 29, 61, 382, 496, 59], [604, 107, 295], [277, 108, 605, 606, 509, 102, 607, 329, 191, 360, 118, 608, 396, 322], [484, 70, 609, 252, 610, 85, 209, 611, 612, 51, 100, 131], [412, 49, 7, 613, 614, 615, 575, 616, 617, 618, 147, 292, 102, 619, 299, 620, 370, 152, 416, 621, 299, 62], [622, 622], [62], [623, 624, 394], [0, 287, 625, 626, 234, 627, 147, 7, 299, 187, 218, 321, 628, 629, 630, 631, 632, 110, 7, 84, 633, 634, 616, 617, 618, 619, 635, 313, 636, 637, 30, 382], [0, 145, 619, 618, 322, 31, 624, 638, 70, 108, 260, 639, 63], [166, 640, 641, 32], [250, 642, 195, 108, 643], [124, 644, 263], [190, 57, 212, 645], [443, 57, 266, 468, 252, 646], [277, 127, 285, 647, 109], [648, 415, 152, 649], [250, 218, 650, 421, 651, 652, 100, 30, 644], [545, 653, 183, 654, 655, 308, 656, 657, 382, 656, 545, 658, 237, 659, 195, 660], [661, 252], [662, 663, 660, 63, 664], [665, 237, 195, 666, 237], [190, 100, 191, 667, 668, 669, 191, 667, 670, 671, 191, 667, 672, 660, 63], [64, 88, 10, 673, 354], [356, 674, 675, 676, 677, 665, 678], [679, 680, 681, 681, 680, 682, 682, 415, 683, 444], [185, 644, 658], [665, 61, 166, 684, 252, 685, 686, 687, 281, 197, 218, 688, 629, 689, 690, 691, 216, 178, 666], [166, 684, 252, 692, 693, 694, 695, 250, 696, 697, 698, 699, 333, 347, 700], [166, 701, 702, 703, 166, 704, 705, 706, 250, 707, 252, 708, 709, 710, 711, 712], [545, 208, 216, 183, 383, 0, 51, 136, 509, 108, 185, 658, 64, 382, 713, 714, 252, 172, 127, 715, 716, 660], [297, 180, 197, 717], [255, 33, 28, 658, 718], [412, 22, 719, 225, 281, 495, 63], [64, 123, 61, 647], [720, 349, 112, 526, 178, 718, 721, 158, 258, 660, 63], [494, 0, 722, 723, 509, 270, 427, 160, 225], [55, 11, 724], [725, 183, 82, 686, 543, 100, 160, 147, 7, 252, 616, 617, 618, 260], [166, 726, 727], [23], [185, 542, 353], [144, 212, 108, 342, 728, 116, 729, 30, 730, 519, 85, 361, 125, 731, 732, 259, 116, 733, 619, 541, 634, 611, 70, 734, 731, 313, 479], [735, 230, 736, 737], [183, 0, 200, 570, 738, 722, 739, 229, 398, 205, 740, 372], [52, 741, 395, 252, 334, 351], [252, 334, 385, 252, 347, 348, 529, 344, 342, 195, 742, 297, 23, 383, 516, 88, 353, 88], [250, 743, 744], [725, 183, 216, 745, 746, 106, 107, 747], [748, 749, 750], [751, 349, 213, 752, 182, 753, 754, 382, 395, 755, 183], [0, 756, 549, 178, 757, 758, 759, 382, 560, 760, 398, 761, 762, 763, 753, 421, 764, 765, 766, 767, 324, 768, 769, 319, 755, 770], [356, 259, 33, 607, 509, 771, 598, 85, 361], [270, 494, 58], [183, 0, 772, 332, 398, 59], [64, 773, 774, 775, 509], [735, 776, 640, 483, 545], [356, 641, 640, 32], [596, 609, 777, 778, 607, 252, 59, 772, 779, 130, 349, 23, 185, 780, 781, 510, 353, 259, 33, 276, 746, 112], [176, 22, 23, 0, 559, 276], [782, 59, 57, 174, 783, 354, 784, 332], [26, 218, 785, 237, 59, 607], [11, 786, 785], [0, 23, 185, 208, 59, 787, 276, 88, 727, 483, 510, 89, 788, 789, 790, 276, 276, 276, 276, 276], [110, 785, 82, 228, 791, 71, 792, 147, 252, 51, 183, 15], [793, 794, 560, 23, 0, 795, 796, 276, 178, 797, 798], [388, 203, 799, 0, 800, 801, 308, 802, 133, 389], [26, 218, 803, 804, 805], [349, 806, 796, 807, 18], [110, 785, 502, 722, 808, 750, 71, 809, 810], [62, 811, 812], [494, 0, 611, 272, 305, 383, 813, 814], [72, 558, 112, 278], [815, 112, 278], [494, 785, 816, 817, 629], [176, 818, 112, 278], [162, 256, 183, 0, 819, 820, 821], [250, 822, 270, 30, 823, 264, 261, 57, 95, 245, 278, 30, 100], [144, 63, 824, 483, 0, 108, 396, 772, 825, 607, 826, 827, 828, 829, 830, 831, 832, 470, 214, 833, 483, 834, 680, 147, 835, 271, 660, 811, 812], [494, 0, 70, 785, 836, 287, 444, 185, 392, 837], [838, 783, 228], [838, 391], [494, 785, 839, 179, 695], [725, 42, 783, 840], [841, 842, 843, 844, 278, 845, 321, 846, 195], [847, 677, 60, 235], [391], [183, 0, 152, 416, 739, 840, 25], [848, 517, 849], [110, 785, 284, 0, 108, 850, 54, 214, 252, 851, 187, 172, 112, 823], [252, 376, 852, 853, 237, 660, 308, 854, 855, 654], [250, 856, 195, 857, 858, 811, 859, 659, 195, 266, 396, 860, 861, 237, 862, 863, 43, 864, 865], [866, 862, 867, 868, 660, 63], [494, 0, 869, 820, 272, 837], [388, 785, 786, 313, 637, 30, 382, 39, 72, 870, 607, 108], [26, 871, 872, 785, 63], [248, 178, 395, 783, 278, 849, 91, 112], [873, 785, 609, 315, 329, 772, 121, 395, 470, 72, 874, 875, 0, 23, 183, 321, 839, 629, 190, 876, 10, 877, 295, 799, 794, 878, 278, 849], [412, 361, 70, 382, 607, 45, 72, 879, 880, 881, 182, 882, 883, 884, 885, 194, 467, 886], [0, 366, 59, 887, 888, 772, 877], [5, 889], [890, 891, 892, 893], [579, 391, 640, 894, 888, 895, 14, 880, 896, 897, 881, 898, 899, 53, 900, 901, 902, 903], [110, 349, 285, 904, 10, 302, 905, 476, 71, 72, 73, 0, 398, 260, 906, 907, 549], [908, 72, 308, 909, 884, 910, 315, 911, 42, 61, 912, 185, 237, 913, 0, 42, 914, 24, 915, 916, 884, 917, 918, 295, 652, 183], [919, 910, 785, 71, 526, 920, 222, 871, 921, 204], [922, 259, 130, 302, 522], [579, 493, 102, 923, 785, 337], [0, 624, 51, 540, 924, 287, 545, 42, 274, 744, 925, 10, 302, 259], [926], [183, 785, 927, 247, 626], [928, 929, 483, 237], [930, 329, 631], [797, 798, 931, 733, 932, 933, 259, 934, 560, 935, 936, 65, 58, 937, 938, 939], [940, 941, 942, 943, 944], [945], [183, 884, 946], [811, 812], [183, 785, 947, 948, 949, 559], [815], [183, 884, 611], [23, 183, 785], [950], [940, 941, 942, 943, 944, 951, 170, 952, 953, 954, 955, 116, 956], [957, 958], [183, 882], [957, 959, 785, 960, 228, 961, 904, 237, 962], [237, 962, 29, 522, 398, 483, 183, 882, 501, 963, 964], [110, 285, 954, 398], [110, 785, 965, 285, 966, 108, 116, 956, 382, 967, 968, 969, 797, 970, 971], [797, 972, 349, 973], [277, 974, 975, 166, 45, 63], [976, 977, 0, 244], [725, 901, 297, 183, 0, 978, 820, 221, 302], [105, 754, 183, 883, 979, 980, 85, 981, 982, 983, 984, 985, 986, 987], [988, 560], [183, 885], [237, 989, 396, 308, 216, 990, 91], [250, 885, 991, 380, 992, 993, 880, 994, 995], [466, 125, 61, 183, 883, 837, 820, 489, 483, 10, 876, 302, 42, 890, 996], [466, 997, 890, 996], [183, 0, 935, 58, 237, 883, 998, 23, 999, 68, 559, 147, 258, 71, 1000, 61, 133], [169, 183, 883, 489, 59, 1001], [250, 273, 112, 665, 483, 1002, 32, 295, 883, 159], [128, 417, 996, 904, 196, 1003, 1004, 1005, 286, 183, 888, 1006, 370, 904], [54, 1007, 519, 340, 313, 218, 1008, 1009, 523, 1009, 1010, 237, 996], [356, 1008, 396, 1011, 72, 302, 883, 48, 871, 110, 996], [879, 329, 1012, 254, 277], [928, 285, 883, 30, 284, 24, 152, 570, 23, 493, 308, 70, 147, 436, 1013, 142, 1014, 1015, 439, 100, 1016, 20, 1017, 475, 1018], [919, 910, 883, 183, 1019, 185, 1020], [277, 573, 1020], [72, 1021, 1022, 1023], [169, 663, 904, 183, 883, 1024, 0, 147, 436, 887, 888, 879, 329, 1025, 1026, 59, 1027], [1027], [0, 213, 1028, 160, 292, 93, 1029, 511, 1030, 1031, 749, 750, 212, 1032, 329, 1020], [54, 1033, 147, 1034, 329], [277, 185, 1035, 237, 183, 785], [1036, 904, 883, 960, 1037], [466, 258, 212, 93], [108, 977, 0], [1038, 1039, 183, 0, 336], [1040, 183, 883], [1041, 879, 329, 883, 979, 1042, 1039, 281, 838, 811, 1043, 1044, 1039, 465, 1045, 1046, 1047], [0, 23, 887, 483, 1048, 82, 1049, 1050, 1051, 30, 57, 133, 61, 1052, 1053, 145, 1039, 894, 1054, 30], [110, 127, 483, 279, 1030, 1055, 1056, 1057, 102, 880, 1058, 30, 450, 353, 1059, 1060, 1061, 395], [356, 910, 493, 923, 1062, 785, 295, 777], [602, 1063, 295, 799, 237, 183, 0, 878, 1064, 1065, 396, 272, 42, 837], [1066, 308, 1067, 1068], [183, 785, 977, 0, 1069], [412, 997, 308, 695, 403, 183, 0, 894, 124, 785, 695, 1070, 1067], [250, 624, 664, 785, 787, 213, 1068, 777, 112], [1071, 183, 276, 735, 1072, 172, 190, 876, 382, 1073, 1074, 818, 495, 95, 1075, 838, 185, 1076, 388, 392, 645, 15], [1077, 276, 1078, 1079, 1076, 63, 1080, 709, 1081, 1082, 42, 1083, 1084], [1082, 1081, 1077, 1085, 916, 1071, 665, 887, 733, 1086, 1087], [602, 1088], [183, 785, 0, 507], [466, 650], [811, 812, 183, 0, 1089, 212, 1090, 1091, 57], [448], [494, 785, 497, 1092], [1093], [183, 0, 843, 626, 130, 863, 894, 540], [62, 677, 627, 1094], [64, 15, 196, 183, 785, 45, 291, 313], [602, 1095, 260, 595], [1096], [1097, 383, 0], [277, 1098, 837, 237]]\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "print([Items_names[_] for _ in Items[0:7]])\n",
    "print(Transactions_list[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "Transactions = np.full((N,M), False, dtype=np.bool)\n",
    "\n",
    "for i, t in enumerate(Transactions_list):\n",
    "    for item in t:\n",
    "        Transactions[i][item] = True\n",
    "\n",
    "# Sanity, print row index 10, 11\n",
    "print(f'{Transactions[10:12].astype(int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data for Weka \n",
    "# Filename = 'C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod11/AliceTxt_weka.csv'\n",
    "\n",
    "# with open(Filename, 'w',newline=\"\") as fout:\n",
    "#     writer = csv.writer(fout, delimiter=',', quoting=csv.QUOTE_ALL, quotechar=\"'\", lineterminator='\\n')\n",
    "#     writer.writerow([Items_names[i] for i in range(M)])\n",
    "#     for i in range(N):\n",
    "#         writer.writerow(list(map(lambda x: '' if x == False else 'True',  Transactions[i])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#weka FPgrowth solution on alice txt\n",
    "#The associations show names mock turtle , white rabit and march hare with the assocation of them saying things\n",
    "\n",
    "\n",
    "theme:       weka.associations.FPGrowth -P 2 -I -1 -N 10 -T 0 -C 0.9 -D 0.05 -U 1.0 -M 0.005\n",
    "Relation:     AliceTxt_weka\n",
    "Instances:    1703\n",
    "Attributes:   2793\n",
    "              [list of attributes omitted]\n",
    "=== Associator model (full training set) ===\n",
    "\n",
    "FPGrowth found 13 rules (displaying top 10)\n",
    "\n",
    " 1. [Mock=True]: 56 ==> [Turtle=True]: 56   <conf:(1)> lift:(29.36) lev:(0.03) conv:(54.09) \n",
    " 2. [White=True]: 22 ==> [Rabbit=True]: 22   <conf:(1)> lift:(39.6) lev:(0.01) conv:(21.44) \n",
    " 3. [Hare=True]: 30 ==> [March=True]: 30   <conf:(1)> lift:(54.94) lev:(0.02) conv:(29.45) \n",
    " 4. [join=True]: 9 ==> [dance=True]: 9   <conf:(1)> lift:(131) lev:(0.01) conv:(8.93) \n",
    " 5. [said=True, Turtle=True]: 32 ==> [Mock=True]: 32   <conf:(1)> lift:(30.41) lev:(0.02) conv:(30.95) \n",
    " 6. [said=True, Mock=True]: 32 ==> [Turtle=True]: 32   <conf:(1)> lift:(29.36) lev:(0.02) conv:(30.91) \n",
    " 7. [said=True, White=True]: 9 ==> [Rabbit=True]: 9   <conf:(1)> lift:(39.6) lev:(0.01) conv:(8.77) \n",
    " 8. [said=True, Hare=True]: 15 ==> [March=True]: 15   <conf:(1)> lift:(54.94) lev:(0.01) conv:(14.73) \n",
    " 9. [March=True]: 31 ==> [Hare=True]: 30   <conf:(0.97)> lift:(54.94) lev:(0.02) conv:(15.23) \n",
    "10. [Turtle=True]: 58 ==> [Mock=True]: 56   <conf:(0.97)> lift:(29.36) lev:(0.03) conv:(18.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "**2. [50 pts] The module class NeuralNetMLP is a single hidden layer neural network implementation. Make the necessary modifications to upgrade it to a 2 hidden layer network. Run it on the MNIST dataset and report its performance.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import array, full, concatenate\n",
    "from numpy.random import ranf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows= 60000, columns= 784\n",
      "Rows= 10000, columns= 784\n"
     ]
    }
   ],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    from numpy import fromfile, uint8\n",
    "    import os\n",
    "    import struct\n",
    "    \n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = fromfile(lbpath, dtype=uint8)\n",
    "        with open(images_path, 'rb') as imgpath:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\",imgpath.read(16))\n",
    "            images = fromfile(imgpath, dtype=uint8).reshape(len(labels), 784)\n",
    "            images = ((images / 255.) - .5) * 2\n",
    "    #\n",
    "    return images, labels\n",
    "\n",
    "X_train, y_train = load_mnist('C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod11/', kind='train')\n",
    "print(f'Rows= {X_train.shape[0]}, columns= {X_train.shape[1]}')\n",
    "\n",
    "X_test, y_test = load_mnist('C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod11/', kind='t10k')\n",
    "print(f'Rows= {X_test.shape[0]}, columns= {X_test.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random, zeros, exp, clip, dot, log, sum, argmax, unique, arange, float\n",
    "    \n",
    "class NeuralNetMLP(object):\n",
    "    def __init__(self, n_hidden=30, epochs=100, eta=0.001, minibatch_size=1, seed=None):\n",
    "        self.random = random.RandomState(seed)  # used to randomize weights\n",
    "        \n",
    "        # add hidden layer variable \n",
    "        \n",
    "        self.n_hidden = n_hidden  # size of the hidden layer\n",
    "        self.epochs = epochs  # number of iterations\n",
    "        self.eta = eta  # learning rate\n",
    "        self.minibatch_size = minibatch_size  # size of training batch - 1 would not work\n",
    "    \n",
    "    @staticmethod\n",
    "    def onehot(y, n_classes):  # one hot encode the input class y\n",
    "        onehot = zeros((n_classes, y.shape[0]))\n",
    "        for idx, val in enumerate(y.astype(int)):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):  # Eq 1\n",
    "        return 1.0 / (1.0 + exp(-clip(z, -250, 250)))\n",
    "\n",
    "    def _forward(self, X):  # Eq 2\n",
    "        z_h = dot(X, self.w_h)\n",
    "        a_h = self.sigmoid(z_h)\n",
    "        \n",
    "        #add hidden layer\n",
    "        z_h2 = dot(a_h, self.w_h2) # made correction here a_h2-> a_h\n",
    "        a_h2 = self.sigmoid(z_h2)\n",
    "        \n",
    "        z_out = dot(a_h2, self.w_out)# made correction here z_h->z_h2\n",
    "        a_out = self.sigmoid(z_out)\n",
    "        \n",
    "        #add hidden layer variables z_h2, a_h2\n",
    "        return z_h, a_h, z_h2, a_h2, z_out, a_out \n",
    "\n",
    "    @staticmethod\n",
    "    def compute_cost(y_enc, output):  # Eq 4\n",
    "        term1 = -y_enc * (log(output))\n",
    "        term2 = (1.0-y_enc) * log(1.0-output)\n",
    "        cost = sum(term1 - term2)\n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X)\n",
    "        y_pred = argmax(z_out, axis=1)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        import sys\n",
    "        n_output = unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1, size=(self.n_hidden, n_output))\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1, size=(n_features, self.n_hidden))\n",
    "        \n",
    "        # add hidden layer - same size as previous hidden layer\n",
    "        self.w_h2 = self .random.normal(loc=0.0,scale=0.1, size=(self.n_hidden, self.n_hidden))\n",
    "        \n",
    "        y_train_enc = self.onehot(y_train, n_output)  # one-hot encode original y\n",
    "        for i in range(self.epochs):\n",
    "            indices = arange(X_train.shape[0])\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                \n",
    "                # report forward with new second hidden layer output\n",
    "                z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "                \n",
    "                #add delta_h2\n",
    "                sigmoid_derivative_h = a_h * (1.0-a_h)  # Eq 3\n",
    "                sigmoid_derivative_h2 = a_h2 * (1.0-a_h2)  # Eq 3\n",
    "                delta_out = a_out - y_train_enc[batch_idx]  # Eq 5\n",
    "                 #add\n",
    "                delta_h2 = (dot(delta_out, self.w_out.T) * sigmoid_derivative_h2)  # Eq 6) \n",
    "                delta_h = (dot(delta_h2, self.w_h2.T) * sigmoid_derivative_h)  # Eq 6 \n",
    "                \n",
    "                \n",
    "                #add grad w_h2\n",
    "                grad_w_out = dot(a_h2.T, delta_out)  # Eq 7\n",
    "                #add\n",
    "                grad_w_h2 = dot(a_h.T, delta_h2)\n",
    "                grad_w_h = dot(X_train[batch_idx].T, delta_h)  # Eq 8\n",
    "               \n",
    "                \n",
    "                #add W.h2\n",
    "                self.w_out -= self.eta*grad_w_out  # Eq 9\n",
    "                self.w_h -= self.eta*grad_w_h  # Eq 9\n",
    "                #add \n",
    "                self.w_h2 -= self.eta*grad_w_h2\n",
    "                \n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_h2, a_h2, z_out, a_out = self._forward(X_train)\n",
    "            cost = self.compute_cost(y_enc=y_train_enc, output=a_out)\n",
    "            y_train_pred = self.predict(X_train)  # monitoring training progress through reclassification\n",
    "            y_valid_pred = self.predict(X_valid)  # monitoring training progress through validation\n",
    "            train_acc = ((sum(y_train == y_train_pred)).astype(float) / X_train.shape[0])\n",
    "            valid_acc = ((sum(y_valid == y_valid_pred)).astype(float) / X_valid.shape[0])\n",
    "            sys.stderr.write('\\r%d/%d | Cost: %.2f ' '| Train/Valid Acc.: %.2f%%/%.2f%% '%\n",
    "                (i+1, self.epochs, cost, train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "        #\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300/300 | Cost: 7849.39 | Train/Valid Acc.: 98.05%/95.74%  "
     ]
    }
   ],
   "source": [
    "nn = NeuralNetMLP(n_hidden=20, epochs=300, eta=0.0005, minibatch_size=100, seed=1)\n",
    "\n",
    "nn.fit(X_train=X_train[:55000], y_train=y_train[:55000], X_valid=X_train[55000:], y_valid=y_train[55000:]) ;\n",
    "\n",
    "# at n_hidden=20, epochs=300, eta=0.0005, minibatch_size=100, seed=1\n",
    "# 300/300 | Cost: 7849.39 | Train/Valid Acc.: 98.05%/95.74%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 94.82%\n",
      "[[ 957    0    1    1    0    3    7    3    6    2]\n",
      " [   0 1106    7    1    0    1    3    3   14    0]\n",
      " [  10    2  971   11    5    0    3    9   21    0]\n",
      " [   0    1   18  954    0   13    0   11   10    3]\n",
      " [   2    0    4    2  936    0    7    5    1   25]\n",
      " [   8    1    1   29    3  820   11    3   11    5]\n",
      " [   9    3    6    0   10   10  918    0    2    0]\n",
      " [   1    5   11   14   11    0    0  976    2    8]\n",
      " [   5    2    6   16    6    7    5    7  913    7]\n",
      " [   5    4    1   14   36    1    1   10    6  931]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nn.predict(X_test)\n",
    "test_acc = ((sum(y_test == y_pred)).astype(float) / y_test.shape[0])\n",
    "\n",
    "print(f'Accuracy= {test_acc*100:.2f}%')\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
