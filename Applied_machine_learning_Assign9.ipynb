{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christian Sutton\n",
    "Module 9\n",
    "10/31/20   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "**[10 pts] Describe the environment in the Nim learning model.**  \n",
    "\n",
    "The environment consists the rules of the deterministic NIM game. Only pick from 1 pile. Must pick at least 1 item. Last player to pick up the last item wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "**[10 pts] Describe the agent in the Nim learning model.)**  \n",
    "The agent in the NIM game is the Qlearner. The Qlearner tablulates all possible game states and learns the best next move through rewards and penalties in the Qtable during each game state win/ or potential loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3  \n",
    "**[10 pts] Describe the reward and penalty in the Nim learning model.**  \n",
    "The reward is created by updating the table with a reward value in the event that a game is won. \n",
    "a penalty is created when the table is decremented with a penalty value when the current move will lead to a loss in the next game state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4  \n",
    "**[10 pts] How many possible states there could be in the Nim game with a maximum of 10\n",
    "items per pile and 3 piles total?**  \n",
    "There are 11^3 states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5  \n",
    "**[10 pts] How many possible actions there could be in the Nim game with 10 items per pile\n",
    "and 3 piles total?**  \n",
    "\n",
    "Assuming the smallest possible move size, there are 30 possible actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6  \n",
    "**[50 pts] Find a way to improve the provided Nim game learning model. Do you think one\n",
    "can beat the Guru player? (Hint: How about penalizing the losses? Hint: It is indeed\n",
    "possible to find a better solution which improves the way Q-learning updates its Q-table).**  \n",
    "\n",
    "My solution is below, there is some kind bug that isn't allowing the penalties to have an effect on Qlearner winning more versus Guru player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s} {wins['A']}   {b:>8s} {wins['B']}\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "            elif np.max(qtable[st2[0], st2[1], st2[2]])== Reward:\n",
    "                qtable_update(-Reward, st1, move, pile, np.min(qtable[st2[0], st2[1], st2[2]]))\n",
    "            else: \n",
    "                qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nim_qlearn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 games, Qlearner 0       Guru 3\n",
      "10 games, Qlearner 0       Guru 10\n",
      "100 games, Qlearner 1       Guru 99\n",
      "1000 games, Qlearner 3       Guru 997\n",
      "10000 games, Qlearner 50       Guru 9950\n",
      "50000 games, Qlearner 271       Guru 49729\n",
      "100000 games, Qlearner 534       Guru 99466\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    #nim_qlearn(n)\n",
    "    a, b = play_games(n, 'Qlearner', 'Guru')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        #\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            #\n",
    "            print(s, file=fout)\n",
    "#\n",
    "qtable_log('C:/Users/physi/Desktop/AppliedMachineLearning_EN.705.601/Mod9/qtable_debug.txt4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
