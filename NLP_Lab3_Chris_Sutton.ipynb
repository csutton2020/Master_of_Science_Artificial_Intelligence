{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chris Sutton\n",
    "#### \n",
    "#### Lab#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A brief summary of my work:\n",
    "# This week I used regEx to parse and pull out and order my data in my data strucutres. I created a vocabulary\n",
    "# from all word in the document via a set. I stored each document in a list then used collections.counter to provide \n",
    "# a frequency of occurence. created an inverse document table via a list of dict that contained tuples of (DF, (docID, tf)).\n",
    "# while my code for parts a-e executed quickly it ran in to significant run time for (f). I ran some cells for 6hrs+. \n",
    "# I quickly attempted to integrate stop words into my code but I was too committed from the previous sections, this would \n",
    "# have caused a significant rewrite of my code which there wasn't time for. While the code was runing I considered the use\n",
    "# of some recursive seach/sort methods but this also would have caused significant headache in an already defined code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Build in-memory inverted files. Inverted files are the primary data structure to support the efficient determination\n",
    "#### of which documents contain specified terms. The objective of this first phase is to process a corpus, much as in the first\n",
    "#### lab exercise. However, this time you will not only create a dictionary with document frequency counts for terms, but each\n",
    "#### dictionary value should also store a postings list (list of docids and term counts) for that term. Document frequency is\n",
    "#### the number of records in the postings list. If you are judicious about your memory use, you can store all these lists in\n",
    "#### memory for the collections that we will use. For part (a) use the TIME dataset, which is very small. The documents are\n",
    "#### in a single file, one document per line. The format is a tab-separated (TSV) file with \"docid [tab] text\". Some lines may\n",
    "#### be long.\n",
    "\n",
    "#### 2 points: Show the posting list tuples (docid, term count) for up to the first 10 (numerically lowest) docids for\n",
    "#### the terms (a) computer, (b) thailand, and (c) rockets\n",
    "#### 2 points: Print the DF and IDF values for the same three words, as well as the total number of documents that\n",
    "#### you processed from the file.\n",
    "#### 2 points: Report how long it took to process the input file of documents and create the dictionary and postings\n",
    "#### lists (in minutes and seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "stringText = open('time-documents.txt','rt',encoding='utf-8').read()\n",
    "# calulate vocabulary of documents \n",
    "# vocabulary set\n",
    "terms = set(re.findall(r\"\\w+\",stringText, re.MULTILINE))\n",
    "\n",
    "#list of each document\n",
    "listOfDocs= re.findall(r\"^\\d+\\t(.*\\n)\",stringText, re.MULTILINE)\n",
    "#list of document numbers\n",
    "listofDocNumbers = [int(i) for i in re.findall(r\"^\\d+\",stringText, re.MULTILINE)]\n",
    "\n",
    "#count of terms in each document (list of doc term frequecy)\n",
    "docTermCounts = []\n",
    "for l in listOfDocs:\n",
    "    docTermCounts.append(collections.Counter(re.findall(r\"\\w+\",l, re.MULTILINE)))\n",
    "      \n",
    "#build table of DF and (doc, term counts)          \n",
    "table =collections.defaultdict(list)\n",
    "# calculate DF (first element in table list)\n",
    "i=0 # DF counter\n",
    "\n",
    "# # enter edit\n",
    "# for n, t in enumerate(terms):\n",
    "#     for number in range(len(docTermCounts)):\n",
    "#         if t in docTermCounts[number].keys():\n",
    "#             i+=1\n",
    "#             table[t]=[i]\n",
    "#     i=0  \n",
    "    \n",
    "# # list documents and term counts (tuples in table list (document number, term count))\n",
    "# for n, t in enumerate(terms):\n",
    "#     for l in range(len(docTermCounts)):\n",
    "#         if t in docTermCounts[l].keys():\n",
    "#             table[t].append((listofDocNumbers[l],docTermCounts[l][t]))\n",
    "# print(table[\"THE\"])\n",
    "\n",
    "# # end edit\n",
    "\n",
    "# start zip edit\n",
    "i=0 # DF counter\n",
    "for t in terms:\n",
    "    for l in docTermCounts:\n",
    "        if t in l.keys():\n",
    "            i+=1\n",
    "            table[t]=[i]\n",
    "    i=0  \n",
    "# list documents and term counts (tuples in table list (document number, term count))\n",
    "for t in terms:\n",
    "    for l,m in zip(docTermCounts, listofDocNumbers):\n",
    "        if t in l.keys():\n",
    "            table[t].append((m,l[t]))\n",
    "# end zip edit\n",
    "\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(308, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print ten lowest docIds = 'COMPUTER'\n",
    "sorted(table['COMPUTER'][1:11],key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(203, 1),\n",
       " (243, 5),\n",
       " (280, 14),\n",
       " (396, 1),\n",
       " (449, 1),\n",
       " (498, 1),\n",
       " (516, 1),\n",
       " (534, 5),\n",
       " (543, 12),\n",
       " (544, 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print ten lowest docIds = 'THAILAND'\n",
    "sorted(table['THAILAND'][1:11], key=lambda x: x[0]) \n",
    "# table['THAILAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 1),\n",
       " (117, 1),\n",
       " (186, 1),\n",
       " (313, 6),\n",
       " (404, 1),\n",
       " (464, 2),\n",
       " (495, 1),\n",
       " (509, 2),\n",
       " (545, 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print ten lowest docIds = 'ROCKETS'\n",
    "sorted(table['ROCKETS'][1:11], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF (listDoc, listOfTerm_DF_DocID_TF, termString):\n",
    "    return math.log2(len(listDoc)/listOfTerm_DF_DocID_TF[termString][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF : 1   IDF  : 8.72451385311995\n"
     ]
    }
   ],
   "source": [
    "# print DF and IDF = 'COMPUTER'\n",
    "print ('DF :',table['COMPUTER'][0], '  IDF  :' ,IDF(listofDocNumbers, table, 'COMPUTER'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF : 11   IDF  : 5.2650822344826524\n"
     ]
    }
   ],
   "source": [
    "# print DF and IDF = 'THAILAND'\n",
    "print ('DF :',table['THAILAND'][0], '  IDF  :' ,IDF(listofDocNumbers, table, 'THAILAND'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF : 9   IDF  : 5.554588851677638\n"
     ]
    }
   ],
   "source": [
    "# print DF and IDF = 'ROCKETS'\n",
    "print ('DF :',table['ROCKETS'][0], '  IDF  :' ,IDF(listofDocNumbers, table, 'ROCKETS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of documents processed\n",
    "len(listofDocNumbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.428626298904419\n"
     ]
    }
   ],
   "source": [
    "# time to import input file of documents and create the dictionary and postings\n",
    "print( end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Document vector length. To compute cosine scores you need to apply TF/IDF term weights and compute vector\n",
    "#### lengths for both documents and queries. We will start with documents. We do not have a list of all terms contained in\n",
    "#### each document. Instead the inverted file from part (a) has lists of docids for each term. However, there is a simple\n",
    "#### algorithm to compute document vector lengths for all documents in parallel, by iterating over all postings lists a single\n",
    "#### time. You then store (say in a separate hashtable) document vector lengths as a single real number. Here is the efficient\n",
    "#### practical algorithm to compute the document vector length, which is the square-root of the sum of the squares of the\n",
    "#### weights.\n",
    "#### 6 points: Print document vector lengths for the first 10 (numerically lowest) docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docVectorLength(listDoc, listOfTerm_DF_DocID_TF ):\n",
    "    doclens = {}\n",
    "    for term in listOfTerm_DF_DocID_TF:\n",
    "        for docid, tf in listOfTerm_DF_DocID_TF[term][1:]:\n",
    "            weight = tf * IDF(listDoc,listOfTerm_DF_DocID_TF,term)\n",
    "            if docid in doclens.keys():\n",
    "                doclens[docid] += weight **2\n",
    "            else:\n",
    "                doclens[docid] = weight **2\n",
    "#     print(sorted(doclens.items()))\n",
    "    for id,sum in doclens.items():\n",
    "        doclens[id] = math.sqrt(sum)\n",
    "    return doclens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 142.98567609258976),\n",
       " (18, 63.781528783328575),\n",
       " (19, 135.90483657626655),\n",
       " (20, 71.25308668419089),\n",
       " (21, 146.06350621019592),\n",
       " (23, 129.1497104390011),\n",
       " (24, 219.14554737046032),\n",
       " (25, 71.20475021266738),\n",
       " (26, 117.39747064506734),\n",
       " (27, 73.94031818838077)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docVector= docVectorLength(listofDocNumbers, table)\n",
    "sorted(docVectorLength(listofDocNumbers, table).items(), key=lambda item: item[0])[:10]\n",
    "# print(docVectorLength(listofDocNumbers, table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Query representation. Read in the file of queries (also a tab-separated file \"queryid [tab] text\"). Tokenize the text\n",
    "#### the same way you tokenized the document collection. Keep term counts (i.e., the query term frequency) for each indexing\n",
    "#### term you find. Note, some terms may not be found in the dictionary -- if so, then just ignore them. Compute a query\n",
    "#### vector length similar to what was done in (b), but this will be easier since you have the entire query vector easily at hand.\n",
    "#### 2 points: Print TF/IDF weights with each query term for the first query (only)\n",
    "#### 2 points: Print the query vector length for the first query (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quertyTf (filePath):\n",
    "    queryStringOfText = open(filePath,'rt',encoding='utf-8').read()\n",
    "    \n",
    "    #list of each query\n",
    "    listOfQuery= re.findall(r\"^\\d+\\t(.*\\n)\",queryStringOfText, re.MULTILINE)\n",
    "    listofQueryNumbers = [int(i) for i in re.findall(r\"^\\d+\",queryStringOfText, re.MULTILINE)]\n",
    "    \n",
    "    #count terms in each query (list of query term frequecy Dict['word']:value)\n",
    "    queryTermCounts = []\n",
    "    for l in listOfQuery:\n",
    "        queryTermCounts.append(collections.Counter(re.findall(r\"\\w+\",l, re.MULTILINE)))\n",
    "    return queryTermCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query vector length of first query = 11.847005038012957\n",
      "\n",
      "qtf*docIDF = [3.169925001442312, 4.4765863396763645, 3.7245138531199498, 0.13955135239879357, 4.402585758232587, 4.402585758232587, 4.200951897062937, 0.003414664412764716, 3.6801197337614964, 0.0, 5.139551352398794]\n"
     ]
    }
   ],
   "source": [
    "listOfWeightsPerQuery=[]\n",
    "queryVectorLength=[]\n",
    "\n",
    "# list of tuple of (word, term frequency)\n",
    "for line in quertyTf('time-queries.txt'):\n",
    "    sqofVectorLengthcomponents=[]\n",
    "    dotproduct=[]\n",
    "    for word in line.items():\n",
    "    #     if query word not in collection vocabulary, skip\n",
    "        if word[0] not in terms: #key = word from query in vocabulary\n",
    "            pass\n",
    "        else:\n",
    "    # tf*IDF dot product store in list \n",
    "            dotproduct.append(word[1]*IDF(listofDocNumbers, table, word[0]))\n",
    "    # store vector length components squared \n",
    "            sqofVectorLengthcomponents.append((word[1]*IDF(listofDocNumbers, table, word[0]))**2)\n",
    "#     print('qtf*docIDF =',dotproduct)\n",
    "# caputre dot product components for a line of query\n",
    "    listOfWeightsPerQuery.append(dotproduct)\n",
    "# caputre query vector length per line of query\n",
    "    queryVectorLength.append(math.sqrt(sum(sqofVectorLengthcomponents)))\n",
    "    \n",
    "print('query vector length of first query =',queryVectorLength[0]) \n",
    "print('\\nqtf*docIDF =',listOfWeightsPerQuery[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Score documents. Documents in the collection can be scored in parallel by processing each query term one at a time.\n",
    "#### Walk down the posting list for each query term and add a partial score to an accumulator for each seen docid. The partial\n",
    "#### score is increased like this:\n",
    "#### score{docid} += querytf * idf(term) * doctf * idf(term)\n",
    "#### After processing all query terms you have a sum for many docids. Now divide score{docid} by doclens{docid} and the\n",
    "#### query vector length. The resulting score is the cosine value and it should lie between 0 and 1.\n",
    "#### 2 points: Report the time it takes to score all queries (minutes/seconds)\n",
    "#### 8 points: Cosine scores appear correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 =time.time()\n",
    "listOfQueryScore=[]\n",
    "for k in range(len(quertyTf('time-queries.txt'))):\n",
    "    score={} \n",
    "    for word in quertyTf('time-queries.txt')[k].items():\n",
    "#     if query word not in collection vocabulary, skip\n",
    "        if word[0] not in terms:\n",
    "            pass\n",
    "        else:\n",
    "            # tf*IDF dot product store in score dict\n",
    "            for docID, doctf in table[word[0]][1:]:\n",
    "                if docID in score.keys():\n",
    "                    score[docID]+=word[1]*(IDF(listofDocNumbers, table, word[0])**2)*doctf/(docVector[docID]*queryVectorLength[k])\n",
    "                else:\n",
    "                    score[docID]=word[1]*(IDF(listofDocNumbers, table, word[0])**2)*doctf/(docVector[docID]*queryVectorLength[k])\n",
    "    listOfQueryScore.append(score)\n",
    "end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28008270263671875\n"
     ]
    }
   ],
   "source": [
    "print(end2-start2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Ranked List. Sort the scores for docids and build a ranked list of the top 50 docids per query. Remember that a\n",
    "#### higher cosine score is usually a better document. Create an output file named time-YOURJHED.txt that precisely follows\n",
    "#### the following format:\n",
    "#### queryid Q0 docid rank score jhed\n",
    "#### where the six fields are separated by a single space. Here is a description of each of the six required fields:\n",
    "#### queryid: the number in the first field of the input TSV file\n",
    "#### Q0: string literal that is always the same (reads like capital queue zero)\n",
    "#### docid: the numeric id for each document\n",
    "#### rank: starting at 1 (and not more than 50)\n",
    "#### score: cosine score (should be between 0 and 1). No scientific notation allowed. Use 4 to 6 digits of precision.\n",
    "#### jhed: your jhedid (same on each line)\n",
    "####  4 points: Following formatting directions precisely\n",
    "####  4 points: Submit ranked list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eSortedList=[]\n",
    "outputList=[]\n",
    "# sorted(x.items(), key=lambda item: item[1])\n",
    "file1 = open(\"time-CSUTTO16.txt\", \"w\") \n",
    "for x in listOfQueryScore:\n",
    "    eSortedList.append(sorted(x.items(), key=lambda item: item[1], reverse=True)[:50])\n",
    "count=0\n",
    "for q in eSortedList:\n",
    "    count+=1\n",
    "    rank=0\n",
    "    for doc,scor in q:\n",
    "        rank+=1\n",
    "#         print(doc ,scor, count, rank)\n",
    "#         outputList.append((count,'Q0',doc,rank,scor,'csutto16'))\n",
    "        file1.write(str(count)+' '+'Q0'+' '+str(doc)+' '+str(rank)+' '+str(scor)+' '+'csutto16'+'\\n')\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Efficiency. See if your code scales by working on a larger dataset. If you are successful, provide output file fire10-\n",
    "#### jhedid.txt. The FIRE10 data from the class site is a collection of news stories. The data are available in the zipfile. The\n",
    "#### corpus is copyrighted1 and cannot be used for commercial purposes. Note that this collection is larger and the content is\n",
    "#### based around the Indian subcontinent. There are also some oddities - for example, I believe that apostrophe and quote\n",
    "#### characters were replaced with spaces or deleted. There are approximately 120,000 documents (around 300 MB). Note\n",
    "#### that there are missing docids in the collection and the queries file does not start at 1. If you have a problem with this\n",
    "#### larger dataset, don't fret about it too much. Just index as much of it as you can, and describe what you are able to do.\n",
    "#### 2 points: Report time to build (minutes/seconds) as in (a)\n",
    "#### 2 points: Report time to score all queries (minutes/seconds) as in (d)\n",
    "#### 2 points: Provide a reasonable ranked list for these data as in (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start3 = time.time()\n",
    "# import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4e788e0b94d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# calulate vocabulary of documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# vocabulary set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstringText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#list of each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "stringText = open('fire10-documents.txt','rt',encoding='utf-8').read()\n",
    "# calulate vocabulary of documents \n",
    "# vocabulary set\n",
    "terms = set(re.findall(r\"\\w+\",stringText, re.MULTILINE))\n",
    "\n",
    "#list of each document\n",
    "listOfDocs= re.findall(r\"^\\d+\\t(.*\\n)\",stringText, re.MULTILINE)\n",
    "#list of document numbers\n",
    "listofDocNumbers = [int(i) for i in re.findall(r\"^\\d+\",stringText, re.MULTILINE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of terms in each document (list of doc term frequecy)\n",
    "docTermCounts = []\n",
    "for l in listOfDocs:\n",
    "    docTermCounts.append(collections.Counter(re.findall(r\"\\w+\",l, re.MULTILINE)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  6 plus hours run time for this cell until stopped!!!!!  \n",
    "# I wansn't clear from the lab to use NLTK stop words or to remove anything. The assignment said:\n",
    "# try this file and report the times. The run time with my previous code are excessive when used\n",
    "# so I droppped problem (f).\n",
    "# code below is a mirror image of the code above and is what I intended on using. however, the use of stop words \n",
    "# with my current code appears to be a significant rewrite of my previous design\n",
    "\n",
    "\n",
    "#build table of DF and (doc, term counts)          \n",
    "table =collections.defaultdict(list)\n",
    "# calculate DF (first element in table list)\n",
    "i=0 # DF counter\n",
    "for t in terms:\n",
    "    for l in docTermCounts:\n",
    "        if t in l.keys():\n",
    "            i+=1\n",
    "            table[t]=[i]\n",
    "    i=0  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# list documents and term counts (tuples in table list (document number, term count))\n",
    "for t in terms:\n",
    "    for l,m in zip(docTermCounts, listofDocNumbers):\n",
    "        if t in l.keys():\n",
    "            table[t].append((m,l[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docVector= docVectorLength(listofDocNumbers, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfWeightsPerQuery=[]\n",
    "queryVectorLength=[]\n",
    "\n",
    "# list of tuple of (word, term frequency)\n",
    "for line in quertyTf('fire10-queries.txt'):\n",
    "    sqofVectorLengthcomponents=[]\n",
    "    dotproduct=[]\n",
    "    for word in line.items():\n",
    "    #     if query word not in collection vocabulary, skip\n",
    "        if word[0] not in terms: #key = word from query in vocabulary\n",
    "            pass\n",
    "        else:\n",
    "    # tf*IDF dot product store in list \n",
    "            dotproduct.append(word[1]*IDF(listofDocNumbers, table, word[0]))\n",
    "    # store vector length components squared \n",
    "            sqofVectorLengthcomponents.append((word[1]*IDF(listofDocNumbers, table, word[0]))**2)\n",
    "#     print('qtf*docIDF =',dotproduct)\n",
    "# caputre dot product components for a line of query\n",
    "    listOfWeightsPerQuery.append(dotproduct)\n",
    "# caputre query vector length per line of query\n",
    "    queryVectorLength.append(math.sqrt(sum(sqofVectorLengthcomponents)))\n",
    "end3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start4 = time.time()\n",
    "listOfQueryScore=[]\n",
    "for k in range(len(quertyTf('fire10-queries.txt'))):\n",
    "    score={} \n",
    "    for word in quertyTf('fire10-queries.txt')[k].items():\n",
    "#     if query word not in collection vocabulary, skip\n",
    "        if word[0] not in terms:\n",
    "            pass\n",
    "        else:\n",
    "            # tf*IDF dot product store in score dict\n",
    "            for docID, doctf in table[word[0]][1:]:\n",
    "                if docID in score.keys():\n",
    "                    score[docID]+=word[1]*(IDF(listofDocNumbers, table, word[0])**2)*doctf/(docVector[docID]*queryVectorLength[k])\n",
    "                else:\n",
    "                    score[docID]=word[1]*(IDF(listofDocNumbers, table, word[0])**2)*doctf/(docVector[docID]*queryVectorLength[k])\n",
    "    listOfQueryScore.append(score)\n",
    "end4=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eSortedList=[]\n",
    "outputList=[]\n",
    "# sorted(x.items(), key=lambda item: item[1])\n",
    "# file1 = open(\"fire-CSUTTO16.txt\", \"w\") \n",
    "for x in listOfQueryScore:\n",
    "    eSortedList.append(sorted(x.items(), key=lambda item: item[1], reverse=True)[:50])\n",
    "# count=0\n",
    "# for q in eSortedList:\n",
    "#     count+=1\n",
    "#     rank=0\n",
    "#     for doc,scor in q:\n",
    "#         rank+=1\n",
    "# #         print(doc ,scor, count, rank)\n",
    "# #         outputList.append((count,'Q0',doc,rank,scor,'csutto16'))\n",
    "#         file1.write(str(count)+' '+'Q0'+' '+str(doc)+' '+str(rank)+' '+str(scor)+' '+'csutto16'+'\\n')\n",
    "# file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eSortedList[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
