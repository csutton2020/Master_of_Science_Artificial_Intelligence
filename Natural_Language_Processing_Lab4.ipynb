{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chris Sutton\n",
    "#### \n",
    "#### Lab#4Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this lab I engineered features using multiple methods. For problems B & D, I analyised the training data and chose \n",
    "# a path of creating discriminating unigram features. I reviewed the occurence of terms in the positive and negative class\n",
    "# and chose to pick features that occur more frequently in one class or another. The features were built and passed to the \n",
    "# sklearn countVectorizer which, in the case of problems A and D, simply vectorized them. My total number of features in\n",
    "# problems A and D was 200. For problem E chose to use sklearn to read my files directly, bypassing my custom unigram\n",
    "# features to build unigram and bigrams for a total of 500 features. For classifiers I chose to use Naive Bayes (problem A) \n",
    "# and a support vector classifier (problems D,E). While 200 and 500 features is a lot for data consisting of 2k samples, a\n",
    "# lower number of features results in worse DEVset performance. My number of features is targeting ~.80 F1score. This was\n",
    "# an interesting lab, consume reviews and returning sentiment simply based on the two classes of text.  \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (a) Study the training data. Examine train.tsv. Do you see indications of positive or negative sentiment (e.g., words like\n",
    "##### 'good' or 'terrible'). Are reviews balanced or polarized?\n",
    "##### 4 points. Identify some useful features for positive or negative sentiment. Give 10 examples and the relative\n",
    "##### frequency you observe in both classes (e.g., 'good', 33.4% (pos), 7.6% (neg).)\n",
    "##### 2 points. Report any other observations about the data that are helpful for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I processed the data and analyized it to find a difference in the frequency of occurence depending on if they \n",
    "# appeared in the positive or negative class. Below are some features found by taking thr delta of occurence between\n",
    "# classes. I choose 5 that skewed toward the positive class an 5 that skewed toward the negaitve class. While only\n",
    "# 10 features are shown below I used many more in building the sentiment classifier. \n",
    "\n",
    "# One interesting feature to note is\n",
    "# that the exclaimation mark certainly skew toward the positive class and shows that the positive reviewers tend to use \n",
    "# then often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting features are: \n",
    "# ! (.50 pos, .28 neg)\n",
    "# always (.22 pos, .1 neg)\n",
    "# good (.54 pos, .42 neg)\n",
    "# great (.35 pos, .17 neg)\n",
    "# delicious (.14 pos,.03 neg)\n",
    "# n't (.45 pos, .59 neg)\n",
    "# better (.12 pos, .22 neg)\n",
    "# ordered (.14 pos, .23 neg)\n",
    "# bad (.07 pos, .16 neg)\n",
    "# like (.32 pos, .40 neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['The','.',',','I','If'])\n",
    "# nltk.=ownload('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docidTrain= re.findall(r\"\\t(\\w+)\\t\",open('train.tsv','rt',encoding='utf-8').read(), re.MULTILINE)\n",
    "ratingTrain= [int(x) for x in re.findall(r\"^\\d\",open('train.tsv','rt',encoding='utf-8').read(), re.MULTILINE)]\n",
    "strReview= re.findall(r\"\\t.*\\t(.*\\n)\",open('train.tsv','rt',encoding='utf-8').read(), re.MULTILINE)\n",
    "\n",
    "dociddev= re.findall(r\"\\t(\\w+)\\t\",open('dev.tsv','rt',encoding='utf-8').read(), re.MULTILINE)\n",
    "ratingdev= [int(x) for x in re.findall(r\"^\\d\",open('dev.tsv','rt',encoding='utf-8').read(), re.MULTILINE)]\n",
    "strdev= re.findall(r\"\\t.*\\t(.*\\n)\",open('dev.tsv','rt',encoding='utf-8').read(), re.MULTILINE)\n",
    "\n",
    "docidTest= re.findall(r\"\\t(\\w+)\\t\",open('test.tsv','rt',encoding='utf-8').read(), re.MULTILINE)\n",
    "ratingTest= [int(x) for x in re.findall(r\"^\\d\",open('test.tsv','rt',encoding='utf-8').read(), re.MULTILINE)]\n",
    "strTest= re.findall(r\"\\t.*\\t(.*\\n)\",open('test.tsv','rt',encoding='utf-8').read(), re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z8DDztUxuIoHYHddDL9zQ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docidTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStop(strReview_):\n",
    "    # remove stop words -----------------------------------------------\n",
    "    review=[]\n",
    "    reviewCount=[]\n",
    "    for r in strReview_:\n",
    "        txt=[]\n",
    "    #     reviewCount.append(collections.Counter(word_tokenize(r)))\n",
    "        for word in word_tokenize(r):\n",
    "            if word not in stop:\n",
    "                txt.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        reviewCount.append(collections.Counter(txt)) #count of term per review for analysis\n",
    "        review.append(txt) # list of string term per review\n",
    "    return review, reviewCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatures(reviewCount_,ratingTrain_, posDelta_, negDelta_):\n",
    "# split data into good versus bad DF terms dict------------------------------------\n",
    "    dfg={} # df of good review terms \n",
    "    dfb={} # df of bad review terms\n",
    "    for r, ra in zip(reviewCount_, ratingTrain_):\n",
    "        dfc={}\n",
    "        for k in r.keys():\n",
    "            if k not in dfc.keys(): \n",
    "                dfc[k]=1 # add 1 for each unique word in current reveiw\n",
    "            else:\n",
    "                pass\n",
    "        if ra ==4:\n",
    "            for ke in dfc.keys(): # build dict of good reviews\n",
    "                if ke not in dfg.keys():\n",
    "                    dfg[ke]=1\n",
    "                else:\n",
    "                    dfg[ke]+=1\n",
    "        else:\n",
    "            for ke in dfc.keys(): #build dict of bad reviews\n",
    "                if ke not in dfb.keys():\n",
    "                    dfb[ke]=1\n",
    "                else:\n",
    "                    dfb[ke]+=1    \n",
    "\n",
    "# find all common keys in good verus bad reviews and remove non-common keys----------------------\n",
    "    c=dfg.keys() ^ dfb.keys()\n",
    "    for key in c:\n",
    "        if key in dfg.keys():\n",
    "            del dfg[key]\n",
    "        else:\n",
    "            del dfb[key]\n",
    "# make features by take the diffence of the difference in occurence between classes----------------------------\n",
    "    features={}\n",
    "    for k in dfg:\n",
    "        if dfg[k]-dfb[k]>posDelta_: #45 #10 #5\n",
    "            features[k]=dfg[k]-dfb[k]\n",
    "    for k in dfg:\n",
    "        if dfb[k]-dfg[k]>negDelta_: #56 #10 #5\n",
    "            features[k]=dfb[k]-dfg[k]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopOutAllOtherWords(review_, features_):\n",
    "# stop out all other word in documents that are not features--------------------------\n",
    "    out=[]\n",
    "    for r in review_:\n",
    "        l=''\n",
    "        for w in r:\n",
    "            if w in features_.keys():\n",
    "                l+=w+' '\n",
    "        out.append(l) # list of txt output to scikit\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Train a classifier. Using the training partition build a supervised model using a learning model of your choice. We\n",
    "#### suggest a simple bag of words model as a baseline feature representation. Then make predictions for the dev and test\n",
    "#### partitions and write those to a file.\n",
    "#### 4 points. Describe your approach. Include details such as the algorithm used, important parameters, the type\n",
    "#### of features, and the total number of features.\n",
    "#### 2 points. Print out a feature representation for the first document in the dev set. (Skip 'zeros'.)\n",
    "#### 4 points. Print docid [tab] prediction for the first 10 documents in the dev file. Prediction should be 2 or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My approach was to study the data separated by by review and also by class. I counted the DF of terms per review and \n",
    "# summed them per class to understand the imbalance of review terms per class. next I ensured that both classes contained\n",
    "# the same vocabulary by taking the semetric differnece betwen the two classes. From the positive and negative classes\n",
    "# I built feature by looking for descriminating differences between the classed DF of the remaining terms. I selected all\n",
    "# terms with a between class DF greater then 5. These features are then processed by sklearn countVectorizer into an matrix\n",
    "# to the ML models can process. sklearn counter vectorizer does remove some of my intended features. The documentation \n",
    "# states that all puncuation is removed with vectorizing. The resulting number of features is 862. The number of features \n",
    "# directly affects performance of the classifier. \n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call functions to save the variables for later model buildingearn---------------------------------\n",
    "review ,reviewCount=removeStop(strReview)\n",
    "features= makeFeatures(reviewCount,ratingTrain, 20, 20)\n",
    "out= stopOutAllOtherWords(review, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed my selected features into sklearn to vectorize the features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(out)\n",
    "X_train_counts.shape\n",
    "# X_train_counts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sklearn MNNB while iputting class priors [.5, .5]\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNNB= MultinomialNB(class_prior=[.5,.5]).fit(X_train_counts, ratingTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 200)\n",
      "restaurant ) place good enjoyed Chicken good tasty trying menu Great menu told friendly wanted menu \n"
     ]
    }
   ],
   "source": [
    "# process the dev.tsv document in a similar manner so that it will feed into sklearn.\n",
    "# The first document in the dev set is printed and showing only used features\n",
    "reviewDev ,reviewCountDev=removeStop(strdev)\n",
    "outDev= stopOutAllOtherWords(reviewDev, features)\n",
    "count_vectDev = CountVectorizer()\n",
    "XDev = count_vect.transform(outDev)\n",
    "print(XDev.shape)\n",
    "print(outDev[0])\n",
    "# print(XDev.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZSJnW6faaNFQoqq4ALqYg \t 4\n",
      "Rcbv11hm5AYEwZyqYwAvg \t 4\n",
      "rkRTjhu5szaBggeFVcVJlA \t 4\n",
      "dhmeDsQGUS1FXMLs49SWjQ \t 4\n",
      "z9zfIMYmRRCE4ggfOIieEw \t 4\n",
      "Xtb3pGSh39bqcozkBECw \t 2\n",
      "DOUflAGzxLsXG6xOmR1w \t 2\n",
      "0RxCEWURe08CTcZt95F4AQ \t 4\n",
      "MzUg5twEcCyd0X6lBMP2Lg \t 2\n",
      "uNlw2D5CYKk0wjNxLtYw \t 4\n"
     ]
    }
   ],
   "source": [
    "# make prediction on the dev document (first 10 documents)\n",
    "predicted = MNNB.predict(XDev)\n",
    "count=0\n",
    "for i in range(10):\n",
    "       print(dociddev[i],\"\\t\",predicted[i])\n",
    "    \n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Evaluate your predictions. Using your predictions from part (b), compute precision, recall and F1 scores for just the\n",
    "#### positive class over the full dev set. Show the work in your computation (i.e., the numerators and denominators for\n",
    "#### precision and recall). Recall is the percentage of positive predictions (i.e., 4 stars) in the test file that are correctly\n",
    "#### predicted to belong to the positive class. Precision is the percentage of positive predictions in the predictions file which\n",
    "#### are indeed correct according to the test file labels. For reference: F1 = 2*P*R/(P+R).\n",
    "#### 4 points. Calculate and report Precision, Recall, and F1 scores.\n",
    "#### 4 points. Reasonable results (e.g., F1 score > 65). Around 80% is probably not very hard to attain.\n",
    "#### 4 points. Find a few mistakes that the classifier makes. Present a couple of incorrect predictions that you find\n",
    "#### interesting along with a short comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3rd doc) truth=2, predicted=4, features= \"used favorite family recently cut back menu many favorites gone menu online see 's left\"\n",
    "# comment- I think the issue here is the use of favorites and favorite weighting the classifier into a positive review\n",
    "\n",
    "# (10th doc) truth=2, predicted=4, features =\"lunch day little Chinese restaurant also style menu Not many dishes found n't really veggies\n",
    "# dish 2 star\" \n",
    "# comment- beacuse countVectorizer removed all punctuation the (n't), which is frequently used in negative reviews, was\n",
    "# removed which changed the resulting classification \n",
    "\n",
    "# (29th doc) truth=4, predicted=2 \"thought would give $ box try went tonight took go busy obviously $ box got breast\n",
    "# spicy sides beans potato ( : ( ) ... included honest saying far juicy breast crispy beans well You also option 3 \n",
    "# instead 2 piece They busy drive thru No one rude busy After experience location $ meal full place clean : \n",
    "# tables people experience staff gone There sign window\"\n",
    "# comment - I think the number of features appearing in this review coupled with the use of negative words has an effect\n",
    "# on the prediction \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.796\n",
      "   Recall :  0.810\n",
      "  F1Score :  0.803\n"
     ]
    }
   ],
   "source": [
    "# evaluation of precision, recall, F1score of custom features and a MNNB model\n",
    "A=0\n",
    "B=0\n",
    "C=0\n",
    "count=0\n",
    "for truth, p in zip(ratingdev, predicted,):\n",
    "   \n",
    "#     print(count,truth,p)\n",
    "    count+=1\n",
    "    if truth==4 and p==4:\n",
    "        A+=1\n",
    "    elif truth==2 and p==4:\n",
    "        B+=1\n",
    "    elif truth==4 and p==2:\n",
    "        C+=1\n",
    "Precision=A/(A+B)\n",
    "Recall=A/(A+C)\n",
    "print('Precision : ''{: .3f}'.format(A/(A+B)))\n",
    "print('   Recall : ''{: .3f}'.format(A/(A+C)))\n",
    "print('  F1Score : ''{: .3f}'.format(2*Precision*Recall/(Precision+Recall)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Build a second classifier. Repeat steps (b) and (c) using a different machine learning algorithm2.\n",
    "#### (2 points. Provide the same information requested for step (b)\n",
    "#### (2 points. Evaluate and provide the same information from step (c)\n",
    "#### (2 points. Briefly compare your results between the two classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # My approach followed the same methology from above. The feature are also processed by sklearn countVectorizer into a\n",
    "# matrix to the ML models can process. The resulting number of features is 862. Similarly to the NaiveBayes, the number\n",
    "# of features directly affects performance of the classifier. Additionally,the Regularization parameter (C) was chosen to \n",
    "# be .011 by search to achieve the highest precision, recall and F1score.\n",
    "# \n",
    "# (8th doc) truth=4, SVC predicted 2, MNNB predicted 4, features=\"Our Vegas friend drinks late ( ) All needed hear small \n",
    "# bar area place open stopped half priced ( happy ) friend 's friend cool guy working n't want going ordered dishes steak\n",
    "# cheese bread Both good cheese bread really good also one one ' decent got 'd get without creamy tomato sauce 'd say \n",
    "# food 3 stars 's location solid 4 stars \"\n",
    "# comment- nothing really stands out to me as what caused the mis-classifiaction. It could be the words \"late\",\"ordered\"\n",
    "\n",
    "# (113th doc) truth=2, SVC predicted 4, MNNB predicted 2, features=\"Not good definitely better big chain fast food \n",
    "# fries flavor sweet ( ? ) burger `` meh '' n't go back need frozen high \n",
    "# comment- The missing n't due to processign of the countVectorizer may have made a different here. however, the MNNB model\n",
    "# still predicted the correct class. some predictive features for the negative class such as \"better\" appear here but it \n",
    "# didn't seem to help the SVC decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc =LinearSVC( C=.012).fit(X_train_counts, ratingTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "psvc=svc.predict(XDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.799\n",
      "   Recall :  0.792\n",
      "  F1Score :  0.796\n"
     ]
    }
   ],
   "source": [
    "# evaluation of precision, recall, F1score of custom features and a linearSupportVectorClassifier model\n",
    "A=0\n",
    "B=0\n",
    "C=0\n",
    "counter=0\n",
    "for truth, p,pr in zip(ratingdev, psvc,predicted ):\n",
    "#     print(counter,truth,p,pr)\n",
    "    counter+=1\n",
    "    if truth==4 and p==4:\n",
    "        A+=1\n",
    "    elif truth==2 and p==4:\n",
    "        B+=1\n",
    "    elif truth==4 and p==2:\n",
    "        C+=1\n",
    "Precision=A/(A+B)\n",
    "Recall=A/(A+C)\n",
    "print('Precision : ''{: .3f}'.format(A/(A+B)))\n",
    "print('   Recall : ''{: .3f}'.format(A/(A+C)))\n",
    "print('  F1Score : ''{: .3f}'.format(2*Precision*Recall/(Precision+Recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Feature engineering. For one of your classifiers explore using additional features beyond bags of words to improve\n",
    "#### performance. For example, you could use an English sentiment dictionary such as SentiWordNet which is included with\n",
    "#### the lab data. This file requires some pre-processing to extract sentiment terms; Iâve heard that SentiWordNet might be\n",
    "#### available in NLTK. Some other ideas would be to look at word bigrams (e.g., âdelicious foodâ, âhorrible serviceâ),\n",
    "#### handling negation (see J&M chapter 4), use of punctuation (!!!!), or emoticons :-).\n",
    "#### 3 points. Describe any features you added and the effect of using them on the dev set.\n",
    "#### 3 points. In Canvas submit predictions for the test.tsv file using what you think is your best model. The file\n",
    "#### should be named YOURJHED.txt. The format should be docid [tab] prediction. Prediction should be either\n",
    "#### 2 or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For part E I chose to bypass my custom processing used earlier that selected only discriminating terms between classes. \n",
    "# Here I chose to consume the train and dev, test test directly and allow countVectorizer to provide stop words, single\n",
    "# and bigrams of up to 500 features, with a Regularization parameter (C) of .009. I wanted to test the direct performance \n",
    "# of sklearn over my custom term feature generation.It turns out this implementation provides a even balance between \n",
    "# precision and recall and likely we generalize well to test data. \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 500)\n"
     ]
    }
   ],
   "source": [
    "count_vectPartE = CountVectorizer(stop_words='english',ngram_range=(1, 2),max_features=500)\n",
    "X_train_countsPartE = count_vectPartE.fit_transform(strReview)\n",
    "# max_df=.9\n",
    "XDevPartE = count_vectPartE.transform(strdev)\n",
    "print(XDevPartE.shape)\n",
    "svcparte =LinearSVC( C=.009).fit(X_train_countsPartE, ratingTrain)\n",
    "psvcPartE=svcparte.predict(XDevPartE)\n",
    "# print(count_vectPartE.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.805\n",
      "   Recall :  0.797\n",
      "  F1Score :  0.801\n"
     ]
    }
   ],
   "source": [
    "# evaluation of precision, recall, F1score of up to 10K single and bigrams with a linearSupportVectorClassifier model\n",
    "A=0\n",
    "B=0\n",
    "C=0\n",
    "counter=0\n",
    "for truth, p in zip(ratingdev, psvcPartE ):\n",
    "#     print(counter,truth,p)\n",
    "    counter+=1\n",
    "    if truth==4 and p==4:\n",
    "        A+=1\n",
    "    elif truth==2 and p==4:\n",
    "        B+=1\n",
    "    elif truth==4 and p==2:\n",
    "        C+=1\n",
    "Precision=A/(A+B)\n",
    "Recall=A/(A+C)\n",
    "print('Precision : ''{: .3f}'.format(A/(A+B)))\n",
    "print('   Recall : ''{: .3f}'.format(A/(A+C)))\n",
    "print('  F1Score : ''{: .3f}'.format(2*Precision*Recall/(Precision+Recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectPartE = CountVectorizer(stop_words='english',ngram_range=(1, 2),max_features=500)\n",
    "X_train_countsPartE = count_vectPartE.fit_transform(strReview)\n",
    "# # max_df=.9\n",
    "XTestPartE = count_vectPartE.transform(strTest)\n",
    "\n",
    "svcparte =LinearSVC( C=.009).fit(X_train_countsPartE, ratingTrain)\n",
    "psvcPartE=svcparte.predict(XTestPartE)\n",
    "# print(count_vectPartE.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print file docid tab prediction\n",
    "file1 = open(\"CSUTTO16.txt\", \"w\") \n",
    "for doc, pred in zip(docidTest,psvcPartE) :\n",
    "    file1.write(str(doc)+'\\t'+str(pred)+'\\n')\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
