{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chris Sutton\n",
    "### \n",
    "### Lab#2\n",
    "#### \n",
    "#### As of Lab 2 this course is already getting quite interesting with classifing languages based on a trained LM. The key to to completing the Lab was understaning the LM data structure and being able to navigate and parse it to pull in the required probabilites for estimation. I completed the lab using split methods to sort data to my liking but one the last problem I moved over to regEx. I haven't ever had much experience with regEx but it's has come up on some coding entrace tests for the US government so I want to get a better intuition with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "from math import *\n",
    "import pprint\n",
    "import operator\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charlm.py: exmaple code for lab 2 in 605.646\n",
    "\n",
    "# This lab is inspired from a similar lab of Chris Callison-Burch's,\n",
    "# which was in turn inspired by a blot post by Yoav Goldberg:\n",
    "# https://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139\n",
    "#\n",
    "# We fixed the example code to pad each sentence/line with a start indicator instead\n",
    "# of only padding the very first sentence.\n",
    "\n",
    "# Convert counts to probabilities for successor chars in a given context\n",
    "def normalize(counter):\n",
    "        s = float(sum(counter.values()))\n",
    "        return [(c,cnt/s) for c,cnt in counter.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a training file and produce a language model\n",
    "def train_char_lm(fname, order):\n",
    "    data = open(fname,encoding='utf-8').read()\n",
    "    sents = data.split('\\n')\n",
    "    lm = defaultdict(Counter)\n",
    "    for s in sents:\n",
    "        pad = \"~\" * order\n",
    "        data = pad + s + '\\n'\n",
    "#         print(data)\n",
    "        for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "#             print(history, char)\n",
    "            lm[history][char]+=1\n",
    "#             print(lm)\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a character LM, randomly choose a next character given this history and return it\n",
    "def generate_letter(lm, history, order):\n",
    "        history = history[-order:]\n",
    "        dist = lm[history]\n",
    "        x = random()\n",
    "        for c,v in dist:\n",
    "            x = x - v\n",
    "            if x <= 0: return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random text by repeatedly calling generate_letter\n",
    "def generate_text(lm, order, nletters=1000):\n",
    "    history = \"~\" * order\n",
    "    out = []\n",
    "    for i in range(nletters):\n",
    "        c = generate_letter(lm, history, order)\n",
    "        history = history[-order:] + c\n",
    "        out.append(c)\n",
    "        if c == '\\n':\n",
    "            history = \"~\" * order\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print alternatives given this context\n",
    "def print_probs(lm, history):\n",
    "    probs = sorted(lm[history],key=lambda x:(-x[1],x[0]))\n",
    "    pp = pprint.PrettyPrinter()\n",
    "    pp.pprint(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the per-char perplexity of a text, using an input LM.  Returns infinity if a probability isn't found in the model\n",
    "def perplexity(text, lm, order=4):\n",
    "    # Pad the input with \"~\" chars.  This handles the case where order > len(text).\n",
    "    pad = \"~\" * order\n",
    "    data = pad + text\n",
    "    print(data)\n",
    "    result=[]\n",
    "    flag =False\n",
    "    for i in range(len(data)-order):\n",
    "#         print(data[i:i+order])\n",
    "#         print(lm[data[i:i+order]])\n",
    "        if data[i:i+order] not in lm:\n",
    "            flag = True\n",
    "        else:\n",
    "            subsetoflm =lm[data[i:i+order]]\n",
    "        for key, value in subsetoflm:\n",
    "            if key ==data[i+order]:\n",
    "#                 print(key,value)\n",
    "                result.append(log2(value))\n",
    "#     pow(2,-1/len(text)*sum(result)))\n",
    "#         print_probs(lm, data[i:i+order])\n",
    "#         probs = sorted(lm[data[i:i+order]],key=lambda x:(-x[1],x[0]))\n",
    "#     pp = pprint.PrettyPrinter()\n",
    "#     pp.pprint(probs)\n",
    "    # Loop over data string and find probs and use to compute perplexity\n",
    "    if flag ==True:\n",
    "        return inf\n",
    "    else:\n",
    "        return  pow(2,-1/len(text)*sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes per-char perplexity of a text, given an input LM.  Smoothing is very, very simple, just using a small constant\n",
    "def smoothed_perplexity(text, lm, order=4):\n",
    "    # Pad the input with \"~\" chars.  This handles the case where order > len(text).\n",
    "    pad = \"~\" * order\n",
    "    data = pad + text\n",
    "    result=[]\n",
    "    flag =False\n",
    "    for i in range(len(data)-order):\n",
    "        if data[i:i+order] not in lm:\n",
    "            flag = True\n",
    "            result.append(log2(1.0e-7))\n",
    "        else:\n",
    "            subsetoflm =lm[data[i:i+order]]\n",
    "        for key, value in subsetoflm:\n",
    "            if key ==data[i+order]:\n",
    "                result.append(log2(value))\n",
    "    return  pow(2,-1/len(text)*sum(result)) \n",
    "    \n",
    "\n",
    "# end of file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Simple Character LM. Examine the Python code provided (charlm.py). Then train an order 4 (i.e., 5-gram) model from the provided file subtitles.txt. \n",
    "## mylm = train_char_lm('subtitles.txt', 4)\n",
    "### What are the continuations of 'atio' ? What about 'nivi' ? And 'supe' ?\n",
    "### print_probs(mylm, 'atio')\n",
    "### The generate_text method can produce random strings from a trained model. Generate some sample strings (of up to\n",
    "### length 80 characters) from the model. Show us three of your favorite examples produced by the model.\n",
    "### o 3 points: Appropriate continuations for atio, nivi, and supe.\n",
    "### o 1 point: Random sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# troubleshooting, not needed\n",
    "lmT=train_char_lm(\"testTest.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmTrained=train_char_lm(\"subtitles.txt\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n', 0.9940436161014506),\n",
      " (' ', 0.00220962628494572),\n",
      " ('.', 0.0013930252665962147),\n",
      " (',', 0.0009607070804111826),\n",
      " ('?', 0.0003362474781439139),\n",
      " (\"'\", 0.00024017677010279565),\n",
      " ('u', 0.00019214141608223654),\n",
      " ('\"', 0.0001441060620616774),\n",
      " ('s', 0.0001441060620616774),\n",
      " ('-', 9.607070804111827e-05),\n",
      " ('!', 4.8035354020559135e-05),\n",
      " (':', 4.8035354020559135e-05),\n",
      " ('m', 4.8035354020559135e-05),\n",
      " ('p', 4.8035354020559135e-05),\n",
      " ('r', 4.8035354020559135e-05)]\n"
     ]
    }
   ],
   "source": [
    "print_probs(lmTrained, \"atio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n', 0.8), ('e', 0.1), ('s', 0.1)]\n"
     ]
    }
   ],
   "source": [
    "print_probs(lmTrained, \"nivi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r', 0.9992144540455616), ('s', 0.0007855459544383347)]\n"
     ]
    }
   ],
   "source": [
    "print_probs(lmTrained, \"supe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'See?\\nWhat.\\nHow about that lazy.\\n- Diana German.\\n\"There\\'s like you think I\\'m immeasure you want?\\nLook, hey.\\nNo.\\nI know what come?\\nYou don\\'t won with us.\\n- We\\'ll somebody dropped.\\nGet that is well woman'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lmTrained, 4, nletters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All redness sense.\\nA Muslim could cup on the decision positive found the enough Praise shoot my shark?\\nâ€“Sustand I had no idea, but sleep, but of your oath.\\nHell, even the appoint-night?\\nWe shut up. We'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lmTrained, 4, nletters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Wow, eh?\\nI\\'m afraid.\"\\nI\\'LL FINALLY?\\nOrson a prayer asshole later.\\nPrevieve means she\\'s nothing you say.\\nYou\\'re are protocolater it was ride that?\\nHey!\\nHe never.\\nSpills have rebekah one else throw ma'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lmTrained, 4, nletters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are night talking a staff my last this far.\\nNow I reasonalized him ther of Mexicans are best of Calified anything surve arent, Maria striots! Wake it we has been make his is the breason rathere.\\n-'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lmTrained, 4, nletters=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Calculate perplexity. Extend the code from charlm.py to calculate a perplexity score for a provided string against a particular model of a specified order.1 Return positive infinity if any zero probabilities are encountered. \n",
    "### Hint 1: youwill want to work in log space. Hint 2: we advise reading the assigned chapter from the textbook and reviewing the lecture materials before implementing your perplexity function.\n",
    "### perplexity('The boy loves his mother', mylm, 4)\n",
    "### > 3.9092 (your result should be similar)\n",
    "### To test your implementation, provide perplexity values for the following strings using a 5-gram model from subtitles.txt\n",
    "### â€¢ The student loves homework\n",
    "### â€¢ The yob loves homework\n",
    "### â€¢ It is raining in London\n",
    "### â€¢ asdfjkl; qwerty\n",
    "### 2 points: Provides requested test cases\n",
    "### 3 points: Handles zero probabilities\n",
    "### 7 points: Correct implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~The boy loves his mother\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.9091903673746224"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " perplexity('The boy loves his mother', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~The student loves homework\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.606972940490917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity('The student loves homework', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~The yob loves homework\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity('The yob loves homework', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~It is raining in London\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.7112360009044507"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " perplexity('It is raining in London', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~asdfjkl; qwerty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " perplexity('asdfjkl; qwerty', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Naive smoothing. Further extend the code to perform some trivial smoothing for the perplexity function you implemented earlier. Instead of returning positive infinity for zero probabilities, implement a smoothed_perplexity method that uses 1.0e-7 for \"zeros\". As a consequence the sum of all probabilities in your model no longer add up to 1, but the effect will not be very dramatic. Test your method on the same strings from section (b).\n",
    "\n",
    "### 8 points: Working smoothed perplexity and provided requested test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9091903673746224"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_perplexity('The boy loves his mother', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.606972940490917"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_perplexity('The student loves homework', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.992378247505562"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_perplexity('The yob loves homework', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7112360009044507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_perplexity('It is raining in London', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407954.987907739"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_perplexity('asdfjkl; qwerty', lmTrained, order=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Language Identification. Training data for six European languages is provided. Create unigram (history=0) models for all six languages. Then make predictions for the provided test file which contains 1,200 lines. Note: each line in test.txt has two fields separated by a tab character; the first field is the correct language code:\n",
    "### For each input string calculate the smoothed perplexity for the six separate models and return the language code for themodel with lowest smoothed_perplexity. For the first line of the test file (only), show perplexity scores for all sixlanguages. Calculate and report accuracy for the six languages - each number correct should be between 0 and 200 (e.g.,\"de: 180 correct out of 200 lines - 90.0%\"). Finally, repeat the experiment with higher-order n-grams. Try both a bigramand 4-gram model. How do your results change? You'll need to write code to load data, train models, calculate and sortscores, choose the lowest perplexity, and score results for the six languages.\n",
    "### 1 point: Provide scores (perplexities) for each language on first test sentence.\n",
    "### 2 points: Provides predictions for unigram model\n",
    "### 2 points: Predictions for higher order models\n",
    "### 3 points: Quality of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Language_Identification (order, firstSentence):\n",
    "    da=train_char_lm(\"da.train.txt\", order)\n",
    "    de=train_char_lm(\"de.train.txt\", order)\n",
    "    en=train_char_lm(\"en.train.txt\", order)\n",
    "    fr=train_char_lm(\"fr.train.txt\", order)\n",
    "    it=train_char_lm(\"it.train.txt\", order)\n",
    "    nl=train_char_lm(\"nl.train.txt\", order)\n",
    "    langList=[da,de,en,fr,it,nl]\n",
    "    langList2=['da','de','en','fr','it','nl']\n",
    "    solution=[] # returned solution of full test set\n",
    "    solution1=[] # returned solution of one sentence\n",
    "    data= open('test.txt',encoding='utf-8').read()\n",
    "#splitting data up\n",
    "    answerQuestion = data.split('\\n')    \n",
    "    for i in range(len(answerQuestion)):\n",
    "        answerQuestion[i]=answerQuestion[i].split('\\t') \n",
    "# one sentence solution\n",
    "    if firstSentence is True:\n",
    "        resultsOfone={}\n",
    "        for l in range(len(langList)):\n",
    "#             print(langList2[l],smoothed_perplexity(answerQuestion[1][1], langList[l], order))\n",
    "            resultsOfone.update({langList2[l]:smoothed_perplexity(answerQuestion[0][1], langList[l], order)})\n",
    "        solution1.append({key:value for key, value in resultsOfone.items()}) \n",
    "        return solution1\n",
    "#full solution \n",
    "    else:\n",
    "        for i in range(len(answerQuestion)-1):\n",
    "    #         print(answerQuestion[i][1]+'\\n')  \n",
    "            resultsOfTests={}\n",
    "            for l in range(len(langList)):\n",
    "#                 print(langList2[l],smoothed_perplexity(answerQuestion[i][1], langList[l], order))\n",
    "#dict of 5 languages against one sentence\n",
    "                resultsOfTests.update({langList2[l]:smoothed_perplexity(answerQuestion[i][1], langList[l], order)})\n",
    "# list of best performing language per sentnece\n",
    "            solution.append({key:value for key, value in resultsOfTests.items() if value ==min(resultsOfTests.values())}) \n",
    "    #         print(solution)\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'da': 28.990266275714863,\n",
       "  'de': 29.17749118527597,\n",
       "  'en': 20.393278329762353,\n",
       "  'fr': 21.23036140889083,\n",
       "  'it': 23.15318659107709,\n",
       "  'nl': 26.3193826897344}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score for each language for first sentence\n",
    "Language_Identification(0,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for each language for entire test set\n",
    "x= Language_Identification(0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score for each language for entire test set\n",
    "x1= Language_Identification(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth_accuracy(order):\n",
    "    x = Language_Identification(order,False)\n",
    "    data= open('test.txt',encoding='utf-8').read()\n",
    "#splitting data up\n",
    "    answerQuestion = data.split('\\n')    \n",
    "    for i in range(len(answerQuestion)):\n",
    "        answerQuestion[i]=answerQuestion[i].split('\\t')\n",
    "    cntda=0\n",
    "    cntde=0\n",
    "    cntfr=0\n",
    "    cnten=0\n",
    "    cntit=0\n",
    "    cntnl=0\n",
    "#     print(len(answerQuestion))\n",
    "#     print(len(x))\n",
    "    for i in range(len(answerQuestion)-1):\n",
    "#         print(list(x[i].keys())[0],answerQuestion[i][0] )\n",
    "        if list(x[i].keys())[0] ==answerQuestion[i][0]:\n",
    "            if answerQuestion[i][0] == 'da':\n",
    "                cntda+=1\n",
    "            elif answerQuestion[i][0] == 'de':\n",
    "                cntde+=1\n",
    "            elif answerQuestion[i][0] == 'fr':\n",
    "                cntfr+=1\n",
    "            elif answerQuestion[i][0] == 'en':\n",
    "                cnten+=1\n",
    "            elif answerQuestion[i][0] == 'it':\n",
    "                cntit+=1\n",
    "            elif answerQuestion[i][0] == 'nl':\n",
    "                cntnl+=1\n",
    "    return {'da':cntda/200, 'de':cntde/200, 'fr':cntfr/200, 'en':cnten/200, 'it':cntit/200, 'nl':cntnl/200}\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'da': 0.185, 'de': 0.505, 'fr': 0.205, 'en': 0.915, 'it': 0.8, 'nl': 0.855}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resulting accuracy of 0 order\n",
    "truth_accuracy(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'da': 1.0, 'de': 0.995, 'fr': 0.995, 'en': 0.995, 'it': 0.995, 'nl': 0.995}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#resulting accuracy of 4 order\n",
    "truth_accuracy(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Gender Bias. For a different classification problem examine this data that contains questions that were asked to male or female tennis professionals after a televised match. Can you predict whether a question is addressed to a male or to a female player? The files are: tennis.{train,test}.txt and there are two fields separated by a tab. Like the language ID task, see how well you can classify male-directed vs. female-directed questions. See which length model works best. What is your best accuracy on the test set?\n",
    "### 2 points: Calculate accuracies for both classes\n",
    "### 3 points: Show results for models of at least three n-gram orders.\n",
    "### 3 points: Quality of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char_lmSports(sents, order):\n",
    "#     data = open(fname,encoding='utf-8').read()\n",
    "    lm = defaultdict(Counter)\n",
    "    for s in sents:\n",
    "        pad = \"~\" * order\n",
    "        data = pad + s + '\\n'\n",
    "#         print(data)\n",
    "        for i in range(len(data)-order):\n",
    "            history, char = data[i:i+order], data[i+order]\n",
    "#             print(history, char)\n",
    "            lm[history][char]+=1\n",
    "#             print(lm)\n",
    "    outlm = {hist:normalize(chars) for hist, chars in lm.items()}\n",
    "    return outlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def Gender_Bias(order):\n",
    "    data= open('tennis.train.txt',encoding='utf-8').read()\n",
    "    resultM = re.findall(r\"^M\\t(.*\\n)\",data, re.MULTILINE)      \n",
    "    resultF = re.findall(r\"^F\\t(.*\\n)\",data, re.MULTILINE)\n",
    "#     print(resultM[1])\n",
    "    M=train_char_lmSports(resultM, order)\n",
    "    F=train_char_lmSports(resultF, order)\n",
    "    \n",
    "    langList=[M,F]\n",
    "    langList2=['M','F']\n",
    "    solution=[] # returned solution of full test set\n",
    "#splitting data up\n",
    "    answerQuestion = data.split('\\n')    \n",
    "    for i in range(len(answerQuestion)):\n",
    "        answerQuestion[i]=answerQuestion[i].split('\\t') \n",
    "#full solution \n",
    "    else:\n",
    "        for i in range(len(answerQuestion)-1):\n",
    "    #         print(answerQuestion[i][1]+'\\n')  \n",
    "            resultsOfTests={}\n",
    "            for l in range(len(langList)):\n",
    "#                 print(langList2[l],smoothed_perplexity(answerQuestion[i][1], langList[l], order))\n",
    "#dict of 2 languages against one sentence\n",
    "                resultsOfTests.update({langList2[l]:smoothed_perplexity(answerQuestion[i][1], langList[l], order)})\n",
    "# list of best performing language per sentnece\n",
    "            solution.append({key:value for key, value in resultsOfTests.items() if value ==min(resultsOfTests.values())}) \n",
    "    #         print(solution)\n",
    "    \n",
    "    cntM=0\n",
    "    cntF=0\n",
    "#     print(len(answerQuestion))\n",
    "#     print(len(x))\n",
    "    for i in range(len(answerQuestion)-1):\n",
    "#         print(list(x[i].keys())[0],answerQuestion[i][0] )\n",
    "        if list(solution[i].keys())[0] ==answerQuestion[i][0]:\n",
    "            if answerQuestion[i][0] == 'M':\n",
    "                cntM+=1\n",
    "            elif answerQuestion[i][0] == 'F':\n",
    "                cntF+=1\n",
    "    return {'M':cntM/39455, 'F':cntF/34264}\n",
    "#         return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': 0.5915853503991889, 'F': 0.7263308428671492}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_Bias(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': 0.6580661513116208, 'F': 0.716407891664721}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_Bias(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': 0.6959827651755164, 'F': 0.7406899369600747}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gender_Bias(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
