{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 - Programming Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Decision Tree using the ID3 Algorithm (**no** pruning or normalized information gain). Use the provided pseudocode. The data is located at (copy link):\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "**Just in case** the UCI repository is down, which happens from time to time, I have included the data and name files on Blackboard.\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "One of the things we did not talk about in the lectures was how to deal with missing values. There are two aspects of the problem here. What do we do with missing values in the training data? What do we do with missing values when doing classifcation?\n",
    "\n",
    "There are a lot of different ways that we can handle this.\n",
    "A common algorithm is to use something like kNN to impute the missing values.\n",
    "We can use conditional probability as well.\n",
    "There are also clever modifications to the Decision Tree algorithm itself that one can make.\n",
    "\n",
    "We're going to do something simpler, given the size of the data set: remove the observations with missing values (\"?\").\n",
    "\n",
    "You must implement the following functions:\n",
    "\n",
    "`train` takes training_data and returns the Decision Tree as a data structure.\n",
    "\n",
    "```\n",
    "def train(training_data):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "`classify` takes a tree produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data).\n",
    "\n",
    "```\n",
    "def classify(tree, observations, labeled=True):\n",
    "    # returns a list of classifications\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application).\n",
    "\n",
    "Following Module 3's assignment, `cross_validate` should print out a table in exactly the same format. What you are looking for here is a consistent evaluation metric cross the folds. Print the error rate to 4 decimal places. **Do not convert to a percentage.**\n",
    "\n",
    "```\n",
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "```\n",
    "\n",
    "This should be a text representation of a decision tree trained on the entire data set (no train/test).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Decision Tree algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. When you are done, apply the Decision Tree algorithm to the entire data set and print out the resulting tree.\n",
    "\n",
    "**Note** Because this assignment has a natural recursive implementation, you should consider using `deepcopy` at the appropriate places.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The function `parse_data` loads the data from the specified file and returns a List of Lists. The outer List is the data set and each element (List) is a specific observation. Each value of an observation is for a particular measurement. This is what we mean by \"tidy\" data.\n",
    "\n",
    "The function also returns the *shuffled* data because the data might have been collected in a particular order that *might* bias training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splits - n folds\n",
    "\n",
    "With n fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. You pick n based on the size of your data set. If you have a small data set--100 observations--and you used n=10, each fold would only have 10 observations. That's probably too small. You want at least 30. At the other extreme, we generally don't use n > 10.\n",
    "\n",
    "With 8124 observations, n = 10 is fine so we will have 10 folds.\n",
    "`create_folds` will take a list (xs) and split it into `n` equal folds with each fold containing one-tenth of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7311"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"homogeneous\"></a>\n",
    "## homogeneous\n",
    "\n",
    "The homogeneous function checks if all the sub-lists have the same class. **Used by**: [id3](#id3)\n",
    "\n",
    "* **data List[List[str]]:** A collection of sub-lists containing the data of the problem\n",
    "* **Returns: bool:** Returns True if all sub-lists same class, otherwise False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogeneous(data):\n",
    "    first = data[0][0]\n",
    "    return all([first == instance[0] for instance in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert homogeneous([['p','x'], ['p','x'], ['p','x']]) == True\n",
    "assert homogeneous([['p','x'], ['e','x'], ['p','x']]) == False\n",
    "assert homogeneous([['p','y'], ['p','y'], ['p','y']]) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"majority_label\"></a>\n",
    "## majority_label\n",
    "\n",
    "The majority_label function measure the class label occurinng most frequently in the data. **Used by**: [id3](#id3)\n",
    "\n",
    "* **data List[List[str]]:** A collection of sub-lists containing the data of the problem\n",
    "* **Returns: str:** Returns str of class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_label(data):\n",
    "    list_of_first = [row[0] for row in data]\n",
    "    counts = Counter(list_of_first)\n",
    "    return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [['p','x'], ['p','x'], ['p','x']]\n",
    "data2 = [['p','x'], ['e','x'], ['p','x']]\n",
    "data3 = [['e','x'], ['e','x'], ['e','x']]\n",
    "data4 = [['p','x'], ['p','x'], ['e','x'], ['e','x']]\n",
    "assert majority_label(data1) == 'p'\n",
    "assert majority_label(data2) == 'p'\n",
    "assert majority_label(data3) == 'e'\n",
    "assert majority_label(data4) == 'p'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrib = {'cap-shape':1, \n",
    "         'cap-surface':2, \n",
    "         'cap-color':3 , \n",
    "         'bruises?':4, \n",
    "         'odor':5, \n",
    "         'gill-attachment':6 , \n",
    "         'gill-spacing':7 , \n",
    "         'gill-size':8, \n",
    "         'gill-color':9, \n",
    "         'stalk-shape':10, \n",
    "         'stalk-root':11, \n",
    "         'stalk-surface-above-ring':12, \n",
    "         'stalk-surface-below-ring':13, \n",
    "         'stalk-color-above-ring':14, \n",
    "         'stalk-color-below-ring':15, \n",
    "         'veil-type':16, \n",
    "         'veil-color':17, \n",
    "         'ring-number':18, \n",
    "         'ring-type':19, \n",
    "         'spore-print-color':20, \n",
    "         'population':21, \n",
    "         'habitat':22}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"entropy\"></a>\n",
    "## entropy\n",
    "computes the entropy of a set of classes or labels. Entropy measures the uncertainty of a set\n",
    " **Used by**: [pick_best_attribute](#pick_best_attribute)\n",
    "\n",
    "* **classes List[str]:** a list of classes\n",
    "* **Returns: float** entropy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(classes):\n",
    "    label_counts = Counter(classes)\n",
    "    entropy = 0.0\n",
    "    total = len(classes)\n",
    "    for count in label_counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert math.isclose(entropy([1, 1, 0, 0,]), 1, abs_tol=1e-2)\n",
    "assert math.isclose(entropy([1, 0, 0, 0,]), .81, abs_tol=1e-2)\n",
    "assert math.isclose(entropy([0, 0, 0, 0,]), 0, abs_tol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"attribute_domain\"></a>\n",
    "## attribute_domain\n",
    "This function computes the frequency of each unique value for a specified attribute in a dataset. **Used by**: [pick_best_attribute](#pick_best_attribute)\n",
    "\n",
    "* **data List[List[str]]:** A collection of sub-lists containing the data of the problem\n",
    "* **attribute int:** column number in the data for which we want to compute the frequency of its values.\n",
    "* **Returns:** A Counter object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_domain(data, attribute):\n",
    "    return Counter([row[attribute] for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert attribute_domain([['a'], ['a'],['b'],['c']], 0) == {'a':2, 'b':1,'c':1}\n",
    "assert attribute_domain([['a'], ['a'],['a'],['c']], 0) == {'a':3, 'c':1}\n",
    "assert attribute_domain([['a'], ['a'],['a'],['a']], 0) == {'a':4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter_data\"></a>\n",
    "## filter_data\n",
    "This function computes the frequency of each unique value for a specified attribute in a dataset. **Used by**: [pick_best_attribute](#pick_best_attribute)\n",
    "\n",
    "* **data List[List[str]]:** A collection of sub-lists containing the data of the problem\n",
    "* **column_index int:** column number to perfom match on\n",
    "* **category str:** category value to match with\n",
    "* **Returns:** List[List[str]] cleaned data that only contains the category value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, column_index, category):\n",
    "    return [row for row in data if row[column_index] == category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert  filter_data([['a'], ['a'],['b'],['c']], 0, 'a') == [['a'],['a']]\n",
    "assert  filter_data([['a'], ['a'],['b'],['c']], 0, 'b') == [['b']]\n",
    "assert  filter_data([['a'], ['a'],['b'],['c']], 0, 'd') == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_attribute(data, attributes):\n",
    "    results_k=[]\n",
    "    results_i=[]\n",
    "    starting_entropy = entropy([row[0] for row in data])\n",
    "    for attribute, value in zip(attributes,attributes.values()):\n",
    "        for k in attribute_domain(data, attribute).keys():\n",
    "            new_data= filter_data(data, attribute, k)\n",
    "            results_k.append(value*entropy([row[0] for row in new_data]))\n",
    "        results_i.append((sum(results_k),attribute))       \n",
    "    return sorted(results_i)[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tree(tree):\n",
    "    # pretty prints the tree\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3( data, attributes, default)\n",
    "    if len(data) == 0\n",
    "        return None\n",
    "    if homogeneous(data)\n",
    "        return data[0][0] # return class label\n",
    "    if len(attributes) == 0\n",
    "        return majority_label(data)\n",
    "    best_attr = pick_best_attribute(data, attributes)[1]\n",
    "    node = new Node( best_attribute) ### needs work ---want to use a dict of dict\n",
    "    default_label = majority_label(data)\n",
    "    for value in attribute_domain(data, attribute).keys()\n",
    "        subset = filter_data(data, best_attr, value)\n",
    "        child = id3( subset, attributes - best_attr, default_label)\n",
    "        add child to node # needs work ---want to use a dict of dict\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    # returns the Decision Tree\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, observations, labeled=True):\n",
    "    # returns a list of classifications\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def evaluate(Training_set_or_test_set, classifcations_results):\n",
    "#         calculates the classification error rate:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data):\n",
    "#     uses 10 fold cross validation (from Module 3!) to train, classify, and evaluate.\n",
    "    T= train(data) \n",
    "    c= classifiy(T, data, labeled=True)\n",
    "    e= evaluate(data, c)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
