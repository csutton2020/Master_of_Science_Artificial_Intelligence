{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9 - Programming Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "For this assignment you will be implementing and evaluating a Naive Bayes Classifier with the same data from last week:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "\n",
    "(You should have downloaded it).\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        No Pandas. The only acceptable libraries in this class are those contained in the `environment.yml`. No OOP, either. You can used Dicts, NamedTuples, etc. as your abstract data type (ADT) for the the tree and nodes.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "You'll first need to calculate all of the necessary probabilities using a `train` function. A flag will control whether or not you use \"+1 Smoothing\" or not. You'll then need to have a `classify` function that takes your probabilities, a List of instances (possibly a list of 1) and returns a List of Tuples. Each Tuple has the best class in the first position and a dict with a key for every possible class label and the associated *normalized* probability. For example, if we have given the `classify` function a list of 2 observations, we would get the following back:\n",
    "\n",
    "```\n",
    "[(\"e\", {\"e\": 0.98, \"p\": 0.02}), (\"p\", {\"e\": 0.34, \"p\": 0.66})]\n",
    "```\n",
    "\n",
    "when calculating the error rate of your classifier, you should pick the class label with the highest probability; you can write a simple function that takes the Dict and returns that class label.\n",
    "\n",
    "As a reminder, the Naive Bayes Classifier generates the *unnormalized* probabilities from the numerator of Bayes Rule:\n",
    "\n",
    "$$P(C|A) \\propto P(A|C)P(C)$$\n",
    "\n",
    "where C is the class and A are the attributes (data). Since the normalizer of Bayes Rule is the *sum* of all possible numerators and you have to calculate them all, the normalizer is just the sum of the probabilities.\n",
    "\n",
    "You will have the same basic functions as the last module's assignment and some of them can be reused or at least repurposed.\n",
    "\n",
    "`train` takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. There are many options including namedtuples and just plain old nested dictionaries. **No OOP**.\n",
    "\n",
    "```\n",
    "def train(training_data, smoothing=True):\n",
    "   # returns the Decision Tree.\n",
    "```\n",
    "\n",
    "The `smoothing` value defaults to True. You should handle both cases.\n",
    "\n",
    "`classify` takes a NBC produced from the function above and applies it to labeled data (like the test set) or unlabeled data (like some new data). (This is not the same `classify` as the pseudocode which classifies only one instance at a time; it can call it though).\n",
    "\n",
    "```\n",
    "def classify(nbc, observations, labeled=True):\n",
    "    # returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "```\n",
    "\n",
    "`evaluate` takes a data set with labels (like the training set or test set) and the classification result and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$\n",
    "\n",
    "Do not use anything else as evaluation metric or the submission will be deemed incomplete, ie, an \"F\". (Hint: accuracy rate is not the error rate!).\n",
    "\n",
    "`cross_validate` takes the data and uses 10 fold cross validation (from Module 3!) to `train`, `classify`, and `evaluate`. **Remember to shuffle your data before you create your folds**. I leave the exact signature of `cross_validate` to you but you should write it so that you can use it with *any* `classify` function of the same form (using higher order functions and partial application). If you did so last time, you can reuse it for this assignment.\n",
    "\n",
    "Following Module 3's discussion, `cross_validate` should print out the fold number and the evaluation metric (error rate) for each fold and then the average value (and the variance). What you are looking for here is a consistent evaluation metric cross the folds. You should print the error rates in terms of percents (ie, multiply the error rate by 100 and add \"%\" to the end).\n",
    "\n",
    "To summarize...\n",
    "\n",
    "Apply the Naive Bayes Classifier algorithm to the Mushroom data set using 10 fold cross validation and the error rate as the evaluation metric. You will do this *twice*. Once with smoothing=True and once with smoothing=False. You should follow up with a brief explanation for the similarities or differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_data(\"agaricus-lepiota-1.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(xs: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(xs), n)\n",
    "    # be careful of generators...\n",
    "    return list(xs[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test(folds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7311"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrib = {'cap-shape':1, \n",
    "         'cap-surface':2, \n",
    "         'cap-color':3 , \n",
    "         'bruises?':4, \n",
    "         'odor':5, \n",
    "         'gill-attachment':6 , \n",
    "         'gill-spacing':7 , \n",
    "         'gill-size':8, \n",
    "         'gill-color':9, \n",
    "         'stalk-shape':10, \n",
    "         'stalk-root':11, \n",
    "         'stalk-surface-above-ring':12, \n",
    "         'stalk-surface-below-ring':13, \n",
    "         'stalk-color-above-ring':14, \n",
    "         'stalk-color-below-ring':15, \n",
    "         'veil-type':16, \n",
    "         'veil-color':17, \n",
    "         'ring-number':18, \n",
    "         'ring-type':19, \n",
    "         'spore-print-color':20, \n",
    "         'population':21, \n",
    "         'habitat':22}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"count\"></a>\n",
    "## count\n",
    "This function computes probability of each feature category per class. **Used by**: [train](#train)\n",
    "\n",
    "* **data List[List[str]]:** A collection of sub-lists containing the data of the problem\n",
    "* **smoothing bool:** flag used to smooth probabilities if true\n",
    "* **Returns:** (NBC) Naive Bayes Classifier dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(data: list[list[str]], smoothing: bool) -> dict:\n",
    "    num_features = len(data[0][1:]) \n",
    "    class_counts = Counter([row[0] for row in data])\n",
    "    \n",
    "    probabilities = {}\n",
    "    for feature in range(1, num_features + 1):\n",
    "        probabilities[feature] = {}\n",
    "        for cls in class_counts:\n",
    "            rows_of_class = [row for row in data if row[0] == cls] # filter rows for current class\n",
    "            counter = Counter([row[feature] for row in rows_of_class]) # Count occurrences of feature\n",
    "            # Convert counts to prob\n",
    "            if smoothing:\n",
    "                prob = {k: (v+1)/(class_counts[cls]+1) for k, v in counter.items()}\n",
    "            else:\n",
    "                prob = {k: v/class_counts[cls] for k, v in counter.items()}\n",
    "            probabilities[feature][cls] = prob\n",
    "    return probabilities, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    [\"p\", \"x\", \"y\"],\n",
    "    [\"p\", \"y\", \"z\"],\n",
    "    [\"e\", \"x\", \"z\"],\n",
    "    [\"e\", \"y\", \"y\"]]\n",
    "data2 = [\n",
    "    [\"p\", \"m\", \"n\"],\n",
    "    [\"e\", \"n\", \"o\"],\n",
    "    [\"p\", \"m\", \"p\"],\n",
    "    [\"e\", \"m\", \"o\"],\n",
    "    [\"e\", \"n\", \"n\"]]\n",
    "\n",
    "assert count(data1, False) == ({ 1: {\"p\": {\"x\": 0.5, \"y\": 0.5},\"e\": {\"x\": 0.5, \"y\": 0.5}},\n",
    "                               2: {\"p\": {\"y\": 0.5, \"z\": 0.5},\"e\": {\"z\": 0.5, \"y\": 0.5}}}, Counter({'p': 2, 'e': 2}))\n",
    "assert count(data1, True) == ({1: {\"p\": {\"x\": 0.6666666666666666, \"y\": 0.6666666666666666},\"e\": {\"x\": 0.6666666666666666, \"y\": 0.6666666666666666}},\n",
    "                              2: {\"p\": {\"y\": 0.6666666666666666, \"z\": 0.6666666666666666},\"e\": {\"z\": 0.6666666666666666, \"y\": 0.6666666666666666}}}, Counter({'p': 2, 'e': 2}))\n",
    "assert count(data2, False) == ({1: {\"p\": {\"m\": 1.0},\"e\": {\"n\": 0.6666666666666666, \"m\": 0.3333333333333333}},\n",
    "                                2: {\"p\": {\"n\": 0.5, \"p\": 0.5},\"e\": {\"o\": 0.6666666666666666, \"n\": 0.3333333333333333}}}, Counter({'p': 2, 'e': 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## train\n",
    "* **training_data: List[List[str]]:** This is the list of data from which to compute Bayesian probabilities. **Used by**: [none](#none) \n",
    "* **smoothing (Optional) bool:**  A boolean parameter which determines whether or not to apply smoothing. By default, it's set to True.\n",
    "\n",
    "* **Return Tuple[Dict,Dict]:** two dicts composed of the probabilites and class counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes training_data and returns a Naive Bayes Classifier (NBC) as a data structure. \n",
    "There are many options including namedtuples and just plain old nested dictionaries\n",
    "'''\n",
    "def train(training_data, smoothing=True) -> Tuple[Dict,Dict]:\n",
    "    return count(training_data, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"probability_of\"></a>\n",
    "## probability_of\n",
    "\n",
    "un-normalized bayes probability of current class\n",
    "\n",
    "* **instance: List[str]:** This is instance of data from which to compute Bayesian probabilities. **Used by**: [nbc](#nbc) \n",
    "* **label: str:**  class label for data set\n",
    "* **value: int:** count of instance of current class\n",
    "* **probs: NBC:** probabilities of trained data\n",
    "\n",
    "* **Return float:** un-normalized probability of current class instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of(instance, label, value, probs):\n",
    "    result=0\n",
    "    for idx, category in zip(range(1, len(instance[1:])+1), instance[1:]):\n",
    "        result.append(probs[0][idx][label][category])\n",
    "    result.append(value/sum(probs[1].values())) # add class prob\n",
    "    return math.prod(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"normalize\"></a>\n",
    "## normalize\n",
    "* **results dict:**: The unnormalized bayesian probabilities of each class. **Used by**: [nbc](#nbc) \n",
    "* **Returns dict:** A new dictionary with the same keys as the input dictionary, but the values are normalized such that they all sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(results: Dict):\n",
    "    total_sum = sum(results.values())\n",
    "    return {key: value / total_sum for key, value in results.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nbc\"></a>\n",
    "## nbc\n",
    "* **probs dict:** dict: The unnormalized bayesian probabilities of each class. **Used by**: [classify](#classify) \n",
    "* **instance: List[str]:** one instance of data\n",
    "* **Returns Tuple:** return tuple of best and the resultng normalized probabilites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbc(probs, instance):\n",
    "    results = {}\n",
    "    for label, value in probs[1].items(): # from counter dict\n",
    "        results[label] = probability_of(instance, label, value, probs) # label: value = (K:v)\n",
    "    results = normalize(results)\n",
    "    best =  max(zip(results.values(), results.keys()))[1]# essentially argmax\n",
    "    return (best, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a list of tuples, the argmax and the raw data as per the pseudocode.\n",
    "def classify(nbc_, observations, labeled=True):\n",
    "    result =[]\n",
    "    for observation in observations:\n",
    "        result.append(nbc(nbc_, observation))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 1 (1830721448.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[142], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 1\n"
     ]
    }
   ],
   "source": [
    "def evaluate():\n",
    "'''\n",
    "    takes a data set with labels (like the training set or test set) \n",
    "    and the classification result and calculates the classification error rate:\n",
    "'''       \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Did you provide output exactly as requested?\n",
    "2. Did you re-execute the entire notebook? (\"Restart Kernel and Rull All Cells...\")\n",
    "3. If you did not complete the assignment or had difficulty please explain what gave you the most difficulty in the Markdown cell below.\n",
    "4. Did you change the name of the file to `jhed_id.ipynb`?\n",
    "\n",
    "Do not submit any other files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
